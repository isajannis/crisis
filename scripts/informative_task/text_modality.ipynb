{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "309e30cd",
   "metadata": {},
   "source": [
    "Preprocesamiento basado en paper:\n",
    "\n",
    "\"we preprocess them by removing stop words, non-ASCII charachters, numbers, URLs and hashtag signs. we also replace all punctuation marks with white spaces\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "34681525",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: nltk in c:\\users\\isaja\\documents\\dcc\\e\\crisis\\env\\lib\\site-packages (3.9.2)\n",
      "Requirement already satisfied: click in c:\\users\\isaja\\documents\\dcc\\e\\crisis\\env\\lib\\site-packages (from nltk) (8.3.0)\n",
      "Requirement already satisfied: joblib in c:\\users\\isaja\\documents\\dcc\\e\\crisis\\env\\lib\\site-packages (from nltk) (1.5.2)\n",
      "Requirement already satisfied: regex>=2021.8.3 in c:\\users\\isaja\\documents\\dcc\\e\\crisis\\env\\lib\\site-packages (from nltk) (2025.9.18)\n",
      "Requirement already satisfied: tqdm in c:\\users\\isaja\\documents\\dcc\\e\\crisis\\env\\lib\\site-packages (from nltk) (4.67.1)\n",
      "Requirement already satisfied: colorama in c:\\users\\isaja\\documents\\dcc\\e\\crisis\\env\\lib\\site-packages (from click->nltk) (0.4.6)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "[notice] A new release of pip is available: 25.2 -> 25.3\n",
      "[notice] To update, run: python.exe -m pip install --upgrade pip\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: tensorflow in c:\\users\\isaja\\documents\\dcc\\e\\crisis\\env\\lib\\site-packages (2.20.0)\n",
      "Requirement already satisfied: absl-py>=1.0.0 in c:\\users\\isaja\\documents\\dcc\\e\\crisis\\env\\lib\\site-packages (from tensorflow) (2.3.1)\n",
      "Requirement already satisfied: astunparse>=1.6.0 in c:\\users\\isaja\\documents\\dcc\\e\\crisis\\env\\lib\\site-packages (from tensorflow) (1.6.3)\n",
      "Requirement already satisfied: flatbuffers>=24.3.25 in c:\\users\\isaja\\documents\\dcc\\e\\crisis\\env\\lib\\site-packages (from tensorflow) (25.9.23)\n",
      "Requirement already satisfied: gast!=0.5.0,!=0.5.1,!=0.5.2,>=0.2.1 in c:\\users\\isaja\\documents\\dcc\\e\\crisis\\env\\lib\\site-packages (from tensorflow) (0.6.0)\n",
      "Requirement already satisfied: google_pasta>=0.1.1 in c:\\users\\isaja\\documents\\dcc\\e\\crisis\\env\\lib\\site-packages (from tensorflow) (0.2.0)\n",
      "Requirement already satisfied: libclang>=13.0.0 in c:\\users\\isaja\\documents\\dcc\\e\\crisis\\env\\lib\\site-packages (from tensorflow) (18.1.1)\n",
      "Requirement already satisfied: opt_einsum>=2.3.2 in c:\\users\\isaja\\documents\\dcc\\e\\crisis\\env\\lib\\site-packages (from tensorflow) (3.4.0)\n",
      "Requirement already satisfied: packaging in c:\\users\\isaja\\documents\\dcc\\e\\crisis\\env\\lib\\site-packages (from tensorflow) (25.0)\n",
      "Requirement already satisfied: protobuf>=5.28.0 in c:\\users\\isaja\\documents\\dcc\\e\\crisis\\env\\lib\\site-packages (from tensorflow) (6.33.0)\n",
      "Requirement already satisfied: requests<3,>=2.21.0 in c:\\users\\isaja\\documents\\dcc\\e\\crisis\\env\\lib\\site-packages (from tensorflow) (2.32.5)\n",
      "Requirement already satisfied: setuptools in c:\\users\\isaja\\documents\\dcc\\e\\crisis\\env\\lib\\site-packages (from tensorflow) (80.9.0)\n",
      "Requirement already satisfied: six>=1.12.0 in c:\\users\\isaja\\documents\\dcc\\e\\crisis\\env\\lib\\site-packages (from tensorflow) (1.17.0)\n",
      "Requirement already satisfied: termcolor>=1.1.0 in c:\\users\\isaja\\documents\\dcc\\e\\crisis\\env\\lib\\site-packages (from tensorflow) (3.1.0)\n",
      "Requirement already satisfied: typing_extensions>=3.6.6 in c:\\users\\isaja\\documents\\dcc\\e\\crisis\\env\\lib\\site-packages (from tensorflow) (4.15.0)\n",
      "Requirement already satisfied: wrapt>=1.11.0 in c:\\users\\isaja\\documents\\dcc\\e\\crisis\\env\\lib\\site-packages (from tensorflow) (2.0.0)\n",
      "Requirement already satisfied: grpcio<2.0,>=1.24.3 in c:\\users\\isaja\\documents\\dcc\\e\\crisis\\env\\lib\\site-packages (from tensorflow) (1.75.1)\n",
      "Requirement already satisfied: tensorboard~=2.20.0 in c:\\users\\isaja\\documents\\dcc\\e\\crisis\\env\\lib\\site-packages (from tensorflow) (2.20.0)\n",
      "Requirement already satisfied: keras>=3.10.0 in c:\\users\\isaja\\documents\\dcc\\e\\crisis\\env\\lib\\site-packages (from tensorflow) (3.11.3)\n",
      "Requirement already satisfied: numpy>=1.26.0 in c:\\users\\isaja\\documents\\dcc\\e\\crisis\\env\\lib\\site-packages (from tensorflow) (2.3.4)\n",
      "Requirement already satisfied: h5py>=3.11.0 in c:\\users\\isaja\\documents\\dcc\\e\\crisis\\env\\lib\\site-packages (from tensorflow) (3.15.1)\n",
      "Requirement already satisfied: ml_dtypes<1.0.0,>=0.5.1 in c:\\users\\isaja\\documents\\dcc\\e\\crisis\\env\\lib\\site-packages (from tensorflow) (0.5.3)\n",
      "Requirement already satisfied: charset_normalizer<4,>=2 in c:\\users\\isaja\\documents\\dcc\\e\\crisis\\env\\lib\\site-packages (from requests<3,>=2.21.0->tensorflow) (3.4.4)\n",
      "Requirement already satisfied: idna<4,>=2.5 in c:\\users\\isaja\\documents\\dcc\\e\\crisis\\env\\lib\\site-packages (from requests<3,>=2.21.0->tensorflow) (3.11)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in c:\\users\\isaja\\documents\\dcc\\e\\crisis\\env\\lib\\site-packages (from requests<3,>=2.21.0->tensorflow) (2.5.0)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in c:\\users\\isaja\\documents\\dcc\\e\\crisis\\env\\lib\\site-packages (from requests<3,>=2.21.0->tensorflow) (2025.10.5)\n",
      "Requirement already satisfied: markdown>=2.6.8 in c:\\users\\isaja\\documents\\dcc\\e\\crisis\\env\\lib\\site-packages (from tensorboard~=2.20.0->tensorflow) (3.9)\n",
      "Requirement already satisfied: pillow in c:\\users\\isaja\\documents\\dcc\\e\\crisis\\env\\lib\\site-packages (from tensorboard~=2.20.0->tensorflow) (12.0.0)\n",
      "Requirement already satisfied: tensorboard-data-server<0.8.0,>=0.7.0 in c:\\users\\isaja\\documents\\dcc\\e\\crisis\\env\\lib\\site-packages (from tensorboard~=2.20.0->tensorflow) (0.7.2)\n",
      "Requirement already satisfied: werkzeug>=1.0.1 in c:\\users\\isaja\\documents\\dcc\\e\\crisis\\env\\lib\\site-packages (from tensorboard~=2.20.0->tensorflow) (3.1.3)\n",
      "Requirement already satisfied: wheel<1.0,>=0.23.0 in c:\\users\\isaja\\documents\\dcc\\e\\crisis\\env\\lib\\site-packages (from astunparse>=1.6.0->tensorflow) (0.45.1)\n",
      "Requirement already satisfied: rich in c:\\users\\isaja\\documents\\dcc\\e\\crisis\\env\\lib\\site-packages (from keras>=3.10.0->tensorflow) (14.2.0)\n",
      "Requirement already satisfied: namex in c:\\users\\isaja\\documents\\dcc\\e\\crisis\\env\\lib\\site-packages (from keras>=3.10.0->tensorflow) (0.1.0)\n",
      "Requirement already satisfied: optree in c:\\users\\isaja\\documents\\dcc\\e\\crisis\\env\\lib\\site-packages (from keras>=3.10.0->tensorflow) (0.17.0)\n",
      "Requirement already satisfied: MarkupSafe>=2.1.1 in c:\\users\\isaja\\documents\\dcc\\e\\crisis\\env\\lib\\site-packages (from werkzeug>=1.0.1->tensorboard~=2.20.0->tensorflow) (3.0.3)\n",
      "Requirement already satisfied: markdown-it-py>=2.2.0 in c:\\users\\isaja\\documents\\dcc\\e\\crisis\\env\\lib\\site-packages (from rich->keras>=3.10.0->tensorflow) (4.0.0)\n",
      "Requirement already satisfied: pygments<3.0.0,>=2.13.0 in c:\\users\\isaja\\documents\\dcc\\e\\crisis\\env\\lib\\site-packages (from rich->keras>=3.10.0->tensorflow) (2.19.2)\n",
      "Requirement already satisfied: mdurl~=0.1 in c:\\users\\isaja\\documents\\dcc\\e\\crisis\\env\\lib\\site-packages (from markdown-it-py>=2.2.0->rich->keras>=3.10.0->tensorflow) (0.1.2)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "[notice] A new release of pip is available: 25.2 -> 25.3\n",
      "[notice] To update, run: python.exe -m pip install --upgrade pip\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: torch in c:\\users\\isaja\\documents\\dcc\\e\\crisis\\env\\lib\\site-packages (2.9.0)\n",
      "Requirement already satisfied: filelock in c:\\users\\isaja\\documents\\dcc\\e\\crisis\\env\\lib\\site-packages (from torch) (3.20.0)\n",
      "Requirement already satisfied: typing-extensions>=4.10.0 in c:\\users\\isaja\\documents\\dcc\\e\\crisis\\env\\lib\\site-packages (from torch) (4.15.0)\n",
      "Requirement already satisfied: sympy>=1.13.3 in c:\\users\\isaja\\documents\\dcc\\e\\crisis\\env\\lib\\site-packages (from torch) (1.14.0)\n",
      "Requirement already satisfied: networkx>=2.5.1 in c:\\users\\isaja\\documents\\dcc\\e\\crisis\\env\\lib\\site-packages (from torch) (3.5)\n",
      "Requirement already satisfied: jinja2 in c:\\users\\isaja\\documents\\dcc\\e\\crisis\\env\\lib\\site-packages (from torch) (3.1.6)\n",
      "Requirement already satisfied: fsspec>=0.8.5 in c:\\users\\isaja\\documents\\dcc\\e\\crisis\\env\\lib\\site-packages (from torch) (2025.9.0)\n",
      "Requirement already satisfied: setuptools in c:\\users\\isaja\\documents\\dcc\\e\\crisis\\env\\lib\\site-packages (from torch) (80.9.0)\n",
      "Requirement already satisfied: mpmath<1.4,>=1.1.0 in c:\\users\\isaja\\documents\\dcc\\e\\crisis\\env\\lib\\site-packages (from sympy>=1.13.3->torch) (1.3.0)\n",
      "Requirement already satisfied: MarkupSafe>=2.0 in c:\\users\\isaja\\documents\\dcc\\e\\crisis\\env\\lib\\site-packages (from jinja2->torch) (3.0.3)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "[notice] A new release of pip is available: 25.2 -> 25.3\n",
      "[notice] To update, run: python.exe -m pip install --upgrade pip\n"
     ]
    }
   ],
   "source": [
    "!pip install nltk\n",
    "!pip install tensorflow\n",
    "!pip install torch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "7854864d",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import re"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "3ff5ae47",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>tweet_id</th>\n",
       "      <th>text_info</th>\n",
       "      <th>text_info_conf</th>\n",
       "      <th>tweet_text</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>917791044158185473</td>\n",
       "      <td>informative</td>\n",
       "      <td>1.0000</td>\n",
       "      <td>RT @Gizmodo: Wildfires raging through Northern...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>917791130590183424</td>\n",
       "      <td>informative</td>\n",
       "      <td>1.0000</td>\n",
       "      <td>PHOTOS: Deadly wildfires rage in California ht...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>917791291823591425</td>\n",
       "      <td>informative</td>\n",
       "      <td>0.6813</td>\n",
       "      <td>RT @Cal_OES: PLS SHARE: Weâ€™re capturing wild...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>917792092100988929</td>\n",
       "      <td>informative</td>\n",
       "      <td>0.6727</td>\n",
       "      <td>RT @TIME: California's raging wildfires as you...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>917792147700465664</td>\n",
       "      <td>informative</td>\n",
       "      <td>0.7143</td>\n",
       "      <td>Wildfires Threaten Californiaâ€™s First Legal ...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "             tweet_id    text_info  text_info_conf  \\\n",
       "0  917791044158185473  informative          1.0000   \n",
       "1  917791130590183424  informative          1.0000   \n",
       "2  917791291823591425  informative          0.6813   \n",
       "3  917792092100988929  informative          0.6727   \n",
       "4  917792147700465664  informative          0.7143   \n",
       "\n",
       "                                          tweet_text  \n",
       "0  RT @Gizmodo: Wildfires raging through Northern...  \n",
       "1  PHOTOS: Deadly wildfires rage in California ht...  \n",
       "2  RT @Cal_OES: PLS SHARE: Weâ€™re capturing wild...  \n",
       "3  RT @TIME: California's raging wildfires as you...  \n",
       "4  Wildfires Threaten Californiaâ€™s First Legal ...  "
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_text = pd.read_csv('../data/crisis_texts_dataset.csv')\n",
    "df_text.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "aae30523",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'did', 'few', 'isn', 'ourselves', \"he's\", 'yours', 'wouldn', 'there', 'when', 'any', 'haven', 's', 'so', 'wasn', 'its', 'for', 'all', 'if', 'up', 'were', 'whom', 'weren', 'had', 'don', 'after', 'd', \"isn't\", 'such', \"mightn't\", 'himself', 've', 'a', 'more', 'to', \"it's\", \"they'd\", 'rt', 'between', 'over', 'ours', 'then', 'with', \"couldn't\", 'doesn', 'by', 'has', 'my', 'off', 'yourself', 'from', 'now', 'again', 'which', \"hasn't\", 'not', \"wouldn't\", 'what', 'under', 'they', \"aren't\", 'that', 'the', 'until', \"you'd\", 'nor', \"they've\", 'was', \"i've\", 'our', 'but', \"won't\", 're', 'why', 'same', 'it', 'each', 'you', \"you've\", \"don't\", 'doing', 'y', 'o', \"you're\", \"we'll\", 'hasn', 'mustn', \"he'd\", \"doesn't\", 'didn', 'shan', 'own', \"we're\", 'is', 'because', 'no', 'their', 'ma', 'where', 'hers', 'yourselves', 'further', 'have', 'we', 'be', 'being', 'who', 'and', \"needn't\", 'this', \"shan't\", \"hadn't\", 'will', 'against', 'herself', 'in', \"she'll\", 'aren', 'shouldn', \"shouldn't\", 'her', 'll', 'he', 'them', 'hadn', 'or', 'RT', 'themselves', \"wasn't\", 'itself', 'most', \"she's\", 'your', 'as', 'below', 'just', \"didn't\", 'other', 'myself', 'of', \"that'll\", 'do', 'am', 'me', 't', 'been', 'm', 'into', \"i'd\", \"they'll\", 'on', \"it'd\", 'some', 'his', 'does', \"she'd\", 'once', 'these', 'during', 'mightn', \"we've\", 'only', 'about', \"should've\", 'having', 'than', \"i'll\", 'couldn', \"weren't\", \"i'm\", 'ain', \"he'll\", 'too', \"mustn't\", 'while', 'can', 'both', 'very', 'before', 'out', 'above', 'here', 'those', 'him', 'needn', 'i', 'should', 'down', 'are', \"haven't\", 'how', 'she', 'at', \"we'd\", 'theirs', \"they're\", 'won', 'through', \"it'll\", \"you'll\", 'an'}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     C:\\Users\\isaja\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "# Para el manejo de stopwords\n",
    "import nltk\n",
    "from nltk.corpus import stopwords\n",
    "nltk.download('stopwords')\n",
    "stop_words = set(stopwords.words('english'))\n",
    "stop_words.update(['rt', 'RT'])\n",
    "print(stop_words)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "28224fcd",
   "metadata": {},
   "outputs": [],
   "source": [
    "def clean_text(text):\n",
    "    # Eliminar URLs\n",
    "    text = re.sub(r'http\\S+|www\\S+|https\\S+', '', text, flags=re.MULTILINE)\n",
    "    # Eliminar caracteres no ASCII\n",
    "    text = text.encode('ascii', 'ignore').decode('ascii')\n",
    "    # Eliminar números\n",
    "    text = re.sub(r'\\d+', '', text)\n",
    "    # Reemplazar signos de puntuación con espacios\n",
    "    text = re.sub(r'[^\\w\\s]', ' ', text)\n",
    "    # Convertir a minúsculas y eliminar stopwords\n",
    "    text = ' '.join([word.lower() for word in text.split() if word.lower() not in stop_words])\n",
    "    return text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "165f6d2c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'alooooooo happy'"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "clean_text(\"RT alooooooo #happy http://example.com 123!!!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "2314c7d0",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>tweet_id</th>\n",
       "      <th>text_info</th>\n",
       "      <th>text_info_conf</th>\n",
       "      <th>tweet_text</th>\n",
       "      <th>clean_text</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>917791044158185473</td>\n",
       "      <td>informative</td>\n",
       "      <td>1.0000</td>\n",
       "      <td>RT @Gizmodo: Wildfires raging through Northern...</td>\n",
       "      <td>gizmodo wildfires raging northern california t...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>917791130590183424</td>\n",
       "      <td>informative</td>\n",
       "      <td>1.0000</td>\n",
       "      <td>PHOTOS: Deadly wildfires rage in California ht...</td>\n",
       "      <td>photos deadly wildfires rage california</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>917791291823591425</td>\n",
       "      <td>informative</td>\n",
       "      <td>0.6813</td>\n",
       "      <td>RT @Cal_OES: PLS SHARE: Weâ€™re capturing wild...</td>\n",
       "      <td>cal_oes pls share capturing wildfire response ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>917792092100988929</td>\n",
       "      <td>informative</td>\n",
       "      <td>0.6727</td>\n",
       "      <td>RT @TIME: California's raging wildfires as you...</td>\n",
       "      <td>time california raging wildfires never seen</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>917792147700465664</td>\n",
       "      <td>informative</td>\n",
       "      <td>0.7143</td>\n",
       "      <td>Wildfires Threaten Californiaâ€™s First Legal ...</td>\n",
       "      <td>wildfires threaten californias first legal can...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>95</th>\n",
       "      <td>917833987824979968</td>\n",
       "      <td>informative</td>\n",
       "      <td>0.6783</td>\n",
       "      <td>Satellite image of California last night. Thos...</td>\n",
       "      <td>satellite image california last night arent li...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>96</th>\n",
       "      <td>917834588814155776</td>\n",
       "      <td>informative</td>\n",
       "      <td>0.6854</td>\n",
       "      <td>RT @AuroraWorldView: Deadly #California #wildf...</td>\n",
       "      <td>auroraworldview deadly california wildfires fo...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>97</th>\n",
       "      <td>917834628290920448</td>\n",
       "      <td>informative</td>\n",
       "      <td>0.6667</td>\n",
       "      <td>Fire swept through Northern California early M...</td>\n",
       "      <td>fire swept northern california early monday mo...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>98</th>\n",
       "      <td>917835067069788162</td>\n",
       "      <td>informative</td>\n",
       "      <td>1.0000</td>\n",
       "      <td>Couple, ages 100 and 98, die in CaliforniaÂ wi...</td>\n",
       "      <td>couple ages die california wildfire</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>99</th>\n",
       "      <td>917835123890049025</td>\n",
       "      <td>informative</td>\n",
       "      <td>0.6436</td>\n",
       "      <td>Napa Fire Map: Napa Valley Timeline, Death Tol...</td>\n",
       "      <td>napa fire map napa valley timeline death toll ...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>100 rows × 5 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "              tweet_id    text_info  text_info_conf  \\\n",
       "0   917791044158185473  informative          1.0000   \n",
       "1   917791130590183424  informative          1.0000   \n",
       "2   917791291823591425  informative          0.6813   \n",
       "3   917792092100988929  informative          0.6727   \n",
       "4   917792147700465664  informative          0.7143   \n",
       "..                 ...          ...             ...   \n",
       "95  917833987824979968  informative          0.6783   \n",
       "96  917834588814155776  informative          0.6854   \n",
       "97  917834628290920448  informative          0.6667   \n",
       "98  917835067069788162  informative          1.0000   \n",
       "99  917835123890049025  informative          0.6436   \n",
       "\n",
       "                                           tweet_text  \\\n",
       "0   RT @Gizmodo: Wildfires raging through Northern...   \n",
       "1   PHOTOS: Deadly wildfires rage in California ht...   \n",
       "2   RT @Cal_OES: PLS SHARE: Weâ€™re capturing wild...   \n",
       "3   RT @TIME: California's raging wildfires as you...   \n",
       "4   Wildfires Threaten Californiaâ€™s First Legal ...   \n",
       "..                                                ...   \n",
       "95  Satellite image of California last night. Thos...   \n",
       "96  RT @AuroraWorldView: Deadly #California #wildf...   \n",
       "97  Fire swept through Northern California early M...   \n",
       "98  Couple, ages 100 and 98, die in CaliforniaÂ wi...   \n",
       "99  Napa Fire Map: Napa Valley Timeline, Death Tol...   \n",
       "\n",
       "                                           clean_text  \n",
       "0   gizmodo wildfires raging northern california t...  \n",
       "1             photos deadly wildfires rage california  \n",
       "2   cal_oes pls share capturing wildfire response ...  \n",
       "3         time california raging wildfires never seen  \n",
       "4   wildfires threaten californias first legal can...  \n",
       "..                                                ...  \n",
       "95  satellite image california last night arent li...  \n",
       "96  auroraworldview deadly california wildfires fo...  \n",
       "97  fire swept northern california early monday mo...  \n",
       "98                couple ages die california wildfire  \n",
       "99  napa fire map napa valley timeline death toll ...  \n",
       "\n",
       "[100 rows x 5 columns]"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_text['clean_text'] = df_text['tweet_text'].apply(clean_text)\n",
    "df_text.head(100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "8c17024b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['taking irma storm',\n",
       " 'live jfkairport awaiting arrival nygovcuomo way puerto rico abcny',\n",
       " 'tesla shows renewable energy project children hospital puerto rico mashabletech',\n",
       " 'antonio mora mind bending illusions',\n",
       " 'sfgate warriors greet cats dogs fled irma oakland airport',\n",
       " 'maria track close perhaps south irma path islands news',\n",
       " 'new mexicans getting word loved ones mexico city earthquake',\n",
       " 'amazing show teamwork unity mexicos earthquake rescue efforts',\n",
       " 'puerto rican doggo helps clean hurricane maria lol aww funny lovely forwhatsapp',\n",
       " 'hhrichardson alliejay genevaathletics genevacollege enjoying golden tornado band home erie']"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Muestra aleatoria de textos limpios\n",
    "import random\n",
    "random.sample(list(df_text['clean_text']), 10)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "55c0d821",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(('informative', 'gizmodo wildfires raging northern california terrifying'),\n",
       " ('informative', 'photos deadly wildfires rage california'),\n",
       " ('informative',\n",
       "  'cal_oes pls share capturing wildfire response recovery info'),\n",
       " ('informative', 'time california raging wildfires never seen'),\n",
       " ('informative',\n",
       "  'wildfires threaten californias first legal cannabis harvest'))"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#dataset = tuple((row['informative'], row['clean_text']) for _, row in df_text.iterrows())\n",
    "dataset = tuple(\n",
    "    (row['text_info'], row['clean_text'])\n",
    "    for _, row in df_text.iterrows() \n",
    ")\n",
    "dataset[:5]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "f0804b8c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tamaño del set de entrenamiento: 12043\n",
      "Tamaño del set de validación: 2408\n",
      "Tamaño del set de prueba: 1607\n",
      "\n",
      "Ejemplos del set de entrenamiento:\n",
      "('informative', 'new photos harvey recovery')\n",
      "('informative', 'image uss iwo jima seen landing craft unit humanitarian assistance efforts following hurricane ir')\n",
      "('informative', 'hurricane irma may bring significant storm surge florida governor says')\n",
      "\n",
      "Ejemplos del set de validación:\n",
      "('informative', 'breakingnews news harvey irma aftermath avoid buying flood damaged car')\n",
      "('informative', 'anaheim hills big fire california fire department great job')\n",
      "('informative', 'gpha_philly successful fundraiser puerto rico hosted sofitelphilly')\n",
      "\n",
      "Ejemplos del set de prueba:\n",
      "('not_informative', 'anything good comes harvey idiot president finally learned thread damn tweets')\n",
      "('informative', 'tornado warnings spike across florida via')\n",
      "('informative', 'delayed due irma congrats talley murphy cheerleader game robinson v plant')\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "from torch.utils.data.dataset import random_split\n",
    "from random import sample\n",
    "\n",
    "# Proporciones\n",
    "train_ratio = 0.75\n",
    "dev_ratio = 0.15\n",
    "test_ratio = 0.15\n",
    "\n",
    "# Longitudes\n",
    "train_len = int(len(dataset) * train_ratio)\n",
    "dev_len = int(len(dataset) * dev_ratio)\n",
    "test_len = len(dataset) - train_len - dev_len  # para evitar problemas de redondeo\n",
    "\n",
    "# Dividimos el dataset\n",
    "train_split, dev_split, test_split = random_split(dataset, [train_len, dev_len, test_len])\n",
    "\n",
    "print(f\"Tamaño del set de entrenamiento: {len(train_split)}\")\n",
    "print(f\"Tamaño del set de validación: {len(dev_split)}\")\n",
    "print(f\"Tamaño del set de prueba: {len(test_split)}\")\n",
    "\n",
    "# Mostramos algunos ejemplos\n",
    "print(\"\\nEjemplos del set de entrenamiento:\")\n",
    "for example in sample(list(train_split), 3):\n",
    "    print(example)\n",
    "print(\"\\nEjemplos del set de validación:\")\n",
    "for example in sample(list(dev_split), 3):\n",
    "    print(example)\n",
    "print(\"\\nEjemplos del set de prueba:\")\n",
    "for example in sample(list(test_split), 3):\n",
    "    print(example)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "c1073207",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt_tab to\n",
      "[nltk_data]     C:\\Users\\isaja\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package punkt_tab is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "# Tokenización\n",
    "nltk.download('punkt_tab')\n",
    "from nltk.tokenize import word_tokenize\n",
    "from collections import namedtuple"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "bc457351",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>text</th>\n",
       "      <th>label</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>[puerto, rico, population, drop, hurricane, ma...</td>\n",
       "      <td>informative</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>[effects, natural, disasters, sri, lanka, incr...</td>\n",
       "      <td>informative</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>[tedfowler, wind, blown, debris, another, hous...</td>\n",
       "      <td>informative</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>[remnants, harvey, spawn, tornadoes, floods, a...</td>\n",
       "      <td>informative</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>[california, wildfire, journalism, success, en...</td>\n",
       "      <td>not_informative</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                text            label\n",
       "0  [puerto, rico, population, drop, hurricane, ma...      informative\n",
       "1  [effects, natural, disasters, sri, lanka, incr...      informative\n",
       "2  [tedfowler, wind, blown, debris, another, hous...      informative\n",
       "3  [remnants, harvey, spawn, tornadoes, floods, a...      informative\n",
       "4  [california, wildfire, journalism, success, en...  not_informative"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "document = namedtuple('tweet', ['text', 'label'])\n",
    "\n",
    "tokenized_train_set = [document(text=word_tokenize(d[1]), label=d[0]) for d in train_split]\n",
    "train_set = pd.DataFrame(tokenized_train_set)\n",
    "\n",
    "tokenized_dev_set = [document(text=word_tokenize(d[1]), label=d[0]) for d in dev_split]\n",
    "dev_set = pd.DataFrame(tokenized_dev_set)\n",
    "\n",
    "tokenized_test_set = [document(text=word_tokenize(d[1]), label=d[0]) for d in test_split]\n",
    "test_set = pd.DataFrame(tokenized_test_set)\n",
    "\n",
    "test_set.head()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "c6fc17c6",
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.keras import layers, models\n",
    "from tensorflow.keras.optimizers import Adam\n",
    "from tensorflow.keras.callbacks import EarlyStopping, ReduceLROnPlateau"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "cffd6c7d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Construcción de nuestro modelo\n",
    "\n",
    "class CrisisCNN:\n",
    "    def __init__(self,\n",
    "                 vocab_size,\n",
    "                 embedding_dim=300,\n",
    "                 max_sequence_length=100,\n",
    "                 num_classes=2):\n",
    "        self.vocab_size = vocab_size\n",
    "        self.embedding_dim = embedding_dim\n",
    "        self.max_sequence_length = max_sequence_length\n",
    "        self.num_classes = num_classes\n",
    "        self.model = self._build_model()\n",
    "    \n",
    "    def _build_model(self):\n",
    "        # Input layer\n",
    "        input_layer = layers.Input(shape=(self.max_sequence_length,))\n",
    "        \n",
    "        # Embedding layer\n",
    "        embedding = layers.Embedding(\n",
    "            input_dim=self.vocab_size,\n",
    "            output_dim=self.embedding_dim,\n",
    "            input_length=self.max_sequence_length,\n",
    "            trainable=False\n",
    "        )(input_layer)\n",
    "        \n",
    "        # Batch normalization after embedding\n",
    "        x = layers.BatchNormalization()(embedding)\n",
    "        \n",
    "        # Parallel convolutional layers\n",
    "        conv_blocks = []\n",
    "        \n",
    "        # 100 filters, window size 2\n",
    "        conv1 = layers.Conv1D(\n",
    "            filters=100,\n",
    "            kernel_size=2,\n",
    "            activation='relu',\n",
    "            padding='same'\n",
    "        )(x)\n",
    "        pool1 = layers.GlobalMaxPooling1D()(conv1)\n",
    "        conv_blocks.append(pool1)\n",
    "        \n",
    "        # 150 filters, window size 3\n",
    "        conv2 = layers.Conv1D(\n",
    "            filters=150,\n",
    "            kernel_size=3,\n",
    "            activation='relu',\n",
    "            padding='same'\n",
    "        )(x)\n",
    "        pool2 = layers.GlobalMaxPooling1D()(conv2)\n",
    "        conv_blocks.append(pool2)\n",
    "        \n",
    "        # 200 filters, window size 4\n",
    "        conv3 = layers.Conv1D(\n",
    "            filters=200,\n",
    "            kernel_size=4,\n",
    "            activation='relu',\n",
    "            padding='same'\n",
    "        )(x)\n",
    "        pool3 = layers.GlobalMaxPooling1D()(conv3)\n",
    "        conv_blocks.append(pool3)\n",
    "        \n",
    "        # Concatenate all convolutional blocks\n",
    "        concatenated = layers.Concatenate()(conv_blocks)\n",
    "        \n",
    "        # Apply dropout\n",
    "        x = layers.Dropout(0.2)(concatenated)\n",
    "        \n",
    "        # 5 Fully connected hidden layers\n",
    "        x = layers.Dense(512, activation='relu')(x)\n",
    "        x = layers.BatchNormalization()(x)\n",
    "        x = layers.Dropout(0.02)(x)\n",
    "        \n",
    "        x = layers.Dense(256, activation='relu')(x)\n",
    "        x = layers.BatchNormalization()(x)\n",
    "        x = layers.Dropout(0.02)(x)\n",
    "        \n",
    "        x = layers.Dense(128, activation='relu')(x)\n",
    "        x = layers.BatchNormalization()(x)\n",
    "        x = layers.Dropout(0.02)(x)\n",
    "        \n",
    "        x = layers.Dense(64, activation='relu')(x)\n",
    "        x = layers.BatchNormalization()(x)\n",
    "        x = layers.Dropout(0.02)(x)\n",
    "        \n",
    "        x = layers.Dense(32, activation='relu')(x)\n",
    "        x = layers.BatchNormalization()(x)\n",
    "        x = layers.Dropout(0.02)(x)\n",
    "        \n",
    "        # Output layer\n",
    "        output_layer = layers.Dense(self.num_classes, activation='softmax')(x)\n",
    "        \n",
    "        model = models.Model(inputs=input_layer, outputs=output_layer)\n",
    "        return model\n",
    "    \n",
    "    def compile_model(self):\n",
    "        self.model.compile(\n",
    "            optimizer=Adam(learning_rate=0.01),\n",
    "            loss='sparse_categorical_crossentropy',\n",
    "            metrics=['accuracy']\n",
    "        )\n",
    "    \n",
    "    def train(self, X_train, y_train, X_val, y_val, epochs=50, batch_size=32):\n",
    "        callbacks = [\n",
    "            EarlyStopping(\n",
    "                monitor='val_accuracy',\n",
    "                patience=10,\n",
    "                restore_best_weights=True,\n",
    "                verbose=1\n",
    "            ),\n",
    "            ReduceLROnPlateau(\n",
    "                monitor='val_loss',\n",
    "                factor=0.5,\n",
    "                patience=5,\n",
    "                min_lr=0.0001,\n",
    "                verbose=1\n",
    "            )\n",
    "        ]\n",
    "        \n",
    "        history = self.model.fit(\n",
    "            X_train, y_train,\n",
    "            validation_data=(X_val, y_val),\n",
    "            epochs=epochs,\n",
    "            batch_size=batch_size,\n",
    "            callbacks=callbacks,\n",
    "            verbose=1\n",
    "        )\n",
    "        \n",
    "        return history\n",
    "\n",
    "    def evaluate(self, X_test, y_test):\n",
    "        return self.model.evaluate(X_test, y_test, verbose=1)\n",
    "\n",
    "    def predict(self, X):\n",
    "        return self.model.predict(X)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "74e7838f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: scikit-learn in c:\\users\\isaja\\documents\\dcc\\e\\crisis\\env\\lib\\site-packages (1.7.2)\n",
      "Requirement already satisfied: numpy>=1.22.0 in c:\\users\\isaja\\documents\\dcc\\e\\crisis\\env\\lib\\site-packages (from scikit-learn) (2.3.4)\n",
      "Requirement already satisfied: scipy>=1.8.0 in c:\\users\\isaja\\documents\\dcc\\e\\crisis\\env\\lib\\site-packages (from scikit-learn) (1.16.2)\n",
      "Requirement already satisfied: joblib>=1.2.0 in c:\\users\\isaja\\documents\\dcc\\e\\crisis\\env\\lib\\site-packages (from scikit-learn) (1.5.2)\n",
      "Requirement already satisfied: threadpoolctl>=3.1.0 in c:\\users\\isaja\\documents\\dcc\\e\\crisis\\env\\lib\\site-packages (from scikit-learn) (3.6.0)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "[notice] A new release of pip is available: 25.2 -> 25.3\n",
      "[notice] To update, run: python.exe -m pip install --upgrade pip\n"
     ]
    }
   ],
   "source": [
    "!pip install scikit-learn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "f74f8c04",
   "metadata": {},
   "outputs": [],
   "source": [
    "import sklearn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "ab502a99",
   "metadata": {},
   "outputs": [],
   "source": [
    "def prepare_sequences_for_cnn(train_set, dev_set, test_set, max_sequence_length=100):\n",
    "    \"\"\"\n",
    "    Prepara secuencias para la CNN desde DataFrames con textos tokenizados\n",
    "    \"\"\"\n",
    "    # Convertir listas de tokens a texto para el tokenizer\n",
    "    def tokens_to_text(token_list):\n",
    "        return ' '.join(token_list)\n",
    "    \n",
    "    # Preparar textos\n",
    "    train_texts = [tokens_to_text(tokens) for tokens in train_set['text']]\n",
    "    dev_texts = [tokens_to_text(tokens) for tokens in dev_set['text']]\n",
    "    test_texts = [tokens_to_text(tokens) for tokens in test_set['text']]\n",
    "    \n",
    "    # Preparar labels (convertir a numéricos si es necesario)\n",
    "    def prepare_labels(labels):\n",
    "        # Si los labels son strings, convertirlos a numéricos\n",
    "        if isinstance(labels.iloc[0], str):\n",
    "            from sklearn.preprocessing import LabelEncoder\n",
    "            le = LabelEncoder()\n",
    "            return le.fit_transform(labels)\n",
    "        return labels.values\n",
    "    \n",
    "    y_train = prepare_labels(train_set['label'])\n",
    "    y_dev = prepare_labels(dev_set['label'])\n",
    "    y_test = prepare_labels(test_set['label'])\n",
    "    \n",
    "    # Crear y ajustar tokenizer\n",
    "    tokenizer = Tokenizer()\n",
    "    tokenizer.fit_on_texts(train_texts)\n",
    "    \n",
    "    # Convertir a secuencias\n",
    "    X_train = tokenizer.texts_to_sequences(train_texts)\n",
    "    X_dev = tokenizer.texts_to_sequences(dev_texts)\n",
    "    X_test = tokenizer.texts_to_sequences(test_texts)\n",
    "    \n",
    "    # Aplicar padding\n",
    "    X_train = pad_sequences(X_train, maxlen=max_sequence_length, padding='post')\n",
    "    X_dev = pad_sequences(X_dev, maxlen=max_sequence_length, padding='post')\n",
    "    X_test = pad_sequences(X_test, maxlen=max_sequence_length, padding='post')\n",
    "    \n",
    "    return X_train, X_dev, X_test, y_train, y_dev, y_test, tokenizer\n",
    "\n",
    "def load_pretrained_embeddings(embedding_path, tokenizer, embedding_dim=300):\n",
    "    \"\"\"\n",
    "    Carga embeddings pre-entrenados word2vec\n",
    "    \"\"\"\n",
    "    embeddings_index = {}\n",
    "    \n",
    "    try:\n",
    "        with open(embedding_path, 'r', encoding='utf-8') as f:\n",
    "            for line in f:\n",
    "                values = line.split()\n",
    "                if len(values) < embedding_dim + 1:\n",
    "                    continue\n",
    "                word = values[0]\n",
    "                coefs = np.asarray(values[1:embedding_dim+1], dtype='float32')\n",
    "                embeddings_index[word] = coefs\n",
    "    except FileNotFoundError:\n",
    "        print(f\"Archivo de embeddings no encontrado: {embedding_path}\")\n",
    "        return None\n",
    "    \n",
    "    # Crear matriz de embeddings\n",
    "    embedding_matrix = np.zeros((len(tokenizer.word_index) + 1, embedding_dim))\n",
    "    words_found = 0\n",
    "    \n",
    "    for word, i in tokenizer.word_index.items():\n",
    "        embedding_vector = embeddings_index.get(word)\n",
    "        if embedding_vector is not None:\n",
    "            embedding_matrix[i] = embedding_vector\n",
    "            words_found += 1\n",
    "    \n",
    "    print(f\"Palabras encontradas en embeddings: {words_found}/{len(tokenizer.word_index)}\")\n",
    "    return embedding_matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "0a3149bf",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "1fd04af2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iniciando experimento CNN...\n",
      "Preparando datos...\n"
     ]
    },
    {
     "ename": "NameError",
     "evalue": "name 'Tokenizer' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mNameError\u001b[39m                                 Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[23]\u001b[39m\u001b[32m, line 51\u001b[39m\n\u001b[32m     48\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m cnn_model, history, tokenizer, test_accuracy\n\u001b[32m     50\u001b[39m \u001b[38;5;28mprint\u001b[39m(\u001b[33m\"\u001b[39m\u001b[33mIniciando experimento CNN...\u001b[39m\u001b[33m\"\u001b[39m)\n\u001b[32m---> \u001b[39m\u001b[32m51\u001b[39m model, history, tokenizer, test_accuracy = \u001b[43mrun_experiment\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m     52\u001b[39m \u001b[43m    \u001b[49m\u001b[43mtrain_set\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\n\u001b[32m     53\u001b[39m \u001b[43m    \u001b[49m\u001b[43mdev_set\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\n\u001b[32m     54\u001b[39m \u001b[43m    \u001b[49m\u001b[43mtest_set\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m     55\u001b[39m \u001b[43m    \u001b[49m\u001b[43membedding_path\u001b[49m\u001b[43m=\u001b[49m\u001b[33;43m'\u001b[39;49m\u001b[33;43mpath/to/word2vec/embeddings.txt\u001b[39;49m\u001b[33;43m'\u001b[39;49m\u001b[43m \u001b[49m\u001b[38;5;66;43;03m# Para usar después (tal vez)\u001b[39;49;00m\n\u001b[32m     56\u001b[39m \u001b[43m)\u001b[49m\n",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[23]\u001b[39m\u001b[32m, line 6\u001b[39m, in \u001b[36mrun_experiment\u001b[39m\u001b[34m(train_set, dev_set, test_set, embedding_path)\u001b[39m\n\u001b[32m      2\u001b[39m \u001b[38;5;250m\u001b[39m\u001b[33;03m\"\"\"\u001b[39;00m\n\u001b[32m      3\u001b[39m \u001b[33;03mEjecuta el experimento completo de CNN\u001b[39;00m\n\u001b[32m      4\u001b[39m \u001b[33;03m\"\"\"\u001b[39;00m\n\u001b[32m      5\u001b[39m \u001b[38;5;28mprint\u001b[39m(\u001b[33m\"\u001b[39m\u001b[33mPreparando datos...\u001b[39m\u001b[33m\"\u001b[39m)\n\u001b[32m----> \u001b[39m\u001b[32m6\u001b[39m X_train, X_dev, X_test, y_train, y_dev, y_test, tokenizer = \u001b[43mprepare_sequences_for_cnn\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m      7\u001b[39m \u001b[43m    \u001b[49m\u001b[43mtrain_set\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdev_set\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtest_set\u001b[49m\n\u001b[32m      8\u001b[39m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m     10\u001b[39m \u001b[38;5;28mprint\u001b[39m(\u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[33mTamaño del vocabulario: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28mlen\u001b[39m(tokenizer.word_index)\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m\"\u001b[39m)\n\u001b[32m     11\u001b[39m \u001b[38;5;28mprint\u001b[39m(\u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[33mForma de X_train: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mX_train.shape\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m\"\u001b[39m)\n",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[21]\u001b[39m\u001b[32m, line 28\u001b[39m, in \u001b[36mprepare_sequences_for_cnn\u001b[39m\u001b[34m(train_set, dev_set, test_set, max_sequence_length)\u001b[39m\n\u001b[32m     25\u001b[39m y_test = prepare_labels(test_set[\u001b[33m'\u001b[39m\u001b[33mlabel\u001b[39m\u001b[33m'\u001b[39m])\n\u001b[32m     27\u001b[39m \u001b[38;5;66;03m# Crear y ajustar tokenizer\u001b[39;00m\n\u001b[32m---> \u001b[39m\u001b[32m28\u001b[39m tokenizer = \u001b[43mTokenizer\u001b[49m()\n\u001b[32m     29\u001b[39m tokenizer.fit_on_texts(train_texts)\n\u001b[32m     31\u001b[39m \u001b[38;5;66;03m# Convertir a secuencias\u001b[39;00m\n",
      "\u001b[31mNameError\u001b[39m: name 'Tokenizer' is not defined"
     ]
    }
   ],
   "source": [
    "def run_experiment(train_set, dev_set, test_set, embedding_path=None):\n",
    "    \"\"\"\n",
    "    Ejecuta el experimento completo de CNN\n",
    "    \"\"\"\n",
    "    print(\"Preparando datos...\")\n",
    "    X_train, X_dev, X_test, y_train, y_dev, y_test, tokenizer = prepare_sequences_for_cnn(\n",
    "        train_set, dev_set, test_set\n",
    "    )\n",
    "    \n",
    "    print(f\"Tamaño del vocabulario: {len(tokenizer.word_index)}\")\n",
    "    print(f\"Forma de X_train: {X_train.shape}\")\n",
    "    print(f\"Forma de y_train: {y_train.shape}\")\n",
    "    \n",
    "    # Crear modelo\n",
    "    print(\"Construyendo modelo CNN...\")\n",
    "    cnn_model = CrisisCNN(\n",
    "        vocab_size=len(tokenizer.word_index) + 1,\n",
    "        embedding_dim=300,\n",
    "        max_sequence_length=100,\n",
    "        num_classes=len(np.unique(y_train))\n",
    "    )\n",
    "    \n",
    "    # Cargar embeddings pre-entrenados si están disponibles\n",
    "    if embedding_path:\n",
    "        print(\"Cargando embeddings pre-entrenados...\")\n",
    "        embedding_matrix = load_pretrained_embeddings(embedding_path, tokenizer)\n",
    "        if embedding_matrix is not None:\n",
    "            cnn_model.model.layers[1].set_weights([embedding_matrix])\n",
    "            print(\"Embeddings pre-entrenados cargados exitosamente\")\n",
    "        else:\n",
    "            print(\"Usando embeddings aleatorios\")\n",
    "    else:\n",
    "        print(\"Usando embeddings aleatorios (entrenables)\")\n",
    "        # Si no hay embeddings pre-entrenados, hacerlos entrenables\n",
    "        cnn_model.model.layers[1].trainable = True\n",
    "    \n",
    "    print(\"Compilando modelo...\")\n",
    "    cnn_model.compile_model()\n",
    "    \n",
    "    print(\"Iniciando entrenamiento...\")\n",
    "    history = cnn_model.train(X_train, y_train, X_dev, y_dev)\n",
    "    \n",
    "    print(\"Evaluando en test set...\")\n",
    "    test_loss, test_accuracy = cnn_model.evaluate(X_test, y_test)\n",
    "    print(f\"Test Accuracy: {test_accuracy:.4f}\")\n",
    "    print(f\"Test Loss: {test_loss:.4f}\")\n",
    "    \n",
    "    return cnn_model, history, tokenizer, test_accuracy\n",
    "\n",
    "print(\"Iniciando experimento CNN...\")\n",
    "model, history, tokenizer, test_accuracy = run_experiment(\n",
    "    train_set, \n",
    "    dev_set, \n",
    "    test_set,\n",
    "    embedding_path='path/to/word2vec/embeddings.txt' # Para usar después (tal vez)\n",
    ")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
