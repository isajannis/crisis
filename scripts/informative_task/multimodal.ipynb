{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "ae351130",
   "metadata": {},
   "source": [
    "## **Multimodal: Text and Image using GloVe and VGG16**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 131,
   "id": "5af2582e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ Usando solo GPU 0\n",
      "✅ GPUs visibles: [PhysicalDevice(name='/physical_device:GPU:0', device_type='GPU'), PhysicalDevice(name='/physical_device:GPU:1', device_type='GPU')]\n"
     ]
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "import os\n",
    "\n",
    "# FORZAR usar solo GPU 0\n",
    "os.environ[\"CUDA_VISIBLE_DEVICES\"] = \"0\"\n",
    "\n",
    "print(\"✅ Usando solo GPU 0\")\n",
    "print(\"✅ GPUs visibles:\", tf.config.list_physical_devices('GPU'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 132,
   "id": "1c8052b6",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "\n",
    "# Configurar TensorFlow para usar menos memoria de GPU\n",
    "os.environ['TF_GPU_ALLOCATOR'] = 'cuda_malloc_async'\n",
    "os.environ['TF_FORCE_GPU_ALLOW_GROWTH'] = 'true'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 109,
   "id": "e4813326",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=== Estado GPU ===\n",
      "✅ GPUs detectadas: 2\n",
      "  - PhysicalDevice(name='/physical_device:GPU:0', device_type='GPU')\n",
      "  - PhysicalDevice(name='/physical_device:GPU:1', device_type='GPU')\n",
      "✅ GPU disponible para uso\n"
     ]
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "import gc\n",
    "import os\n",
    "\n",
    "# Limpiar memoria sin modificar configuraciones de GPU existentes\n",
    "tf.keras.backend.clear_session()\n",
    "gc.collect()\n",
    "\n",
    "# Verificar estado actual de GPU\n",
    "print(\"=== Estado GPU ===\")\n",
    "gpus = tf.config.experimental.list_physical_devices('GPU')\n",
    "if gpus:\n",
    "    print(f\"✅ GPUs detectadas: {len(gpus)}\")\n",
    "    for gpu in gpus:\n",
    "        print(f\"  - {gpu}\")\n",
    "        \n",
    "    # Verificar uso de memoria\n",
    "    try:\n",
    "        from tensorflow.python.client import device_lib\n",
    "        print(f\"✅ GPU disponible para uso\")\n",
    "    except:\n",
    "        print(\"⚠️  No se puede verificar detalles de GPU\")\n",
    "else:\n",
    "    print(\"❌ No se detectaron GPUs\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1ad73c20",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import re\n",
    "import nltk\n",
    "from nltk.corpus import stopwords\n",
    "from gensim.models import Word2Vec\n",
    "import random\n",
    "from tensorflow.keras.utils import to_categorical\n",
    "from PIL import Image\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "08ceb17b",
   "metadata": {},
   "source": [
    "### **Preprocesamiento**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e56406fa",
   "metadata": {},
   "source": [
    "#### **Text**\n",
    "\"we preprocess them by removing stop words, non-ASCII charachters, numbers, URLs and hashtag signs. we also replace all punctuation marks with white spaces\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 111,
   "id": "863cfe6b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>tweet_id</th>\n",
       "      <th>text_info</th>\n",
       "      <th>text_info_conf</th>\n",
       "      <th>tweet_text</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>917791044158185473</td>\n",
       "      <td>informative</td>\n",
       "      <td>1.0000</td>\n",
       "      <td>RT @Gizmodo: Wildfires raging through Northern...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>917791130590183424</td>\n",
       "      <td>informative</td>\n",
       "      <td>1.0000</td>\n",
       "      <td>PHOTOS: Deadly wildfires rage in California ht...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>917791291823591425</td>\n",
       "      <td>informative</td>\n",
       "      <td>0.6813</td>\n",
       "      <td>RT @Cal_OES: PLS SHARE: Weâ€™re capturing wild...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>917792092100988929</td>\n",
       "      <td>informative</td>\n",
       "      <td>0.6727</td>\n",
       "      <td>RT @TIME: California's raging wildfires as you...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>917792147700465664</td>\n",
       "      <td>informative</td>\n",
       "      <td>0.7143</td>\n",
       "      <td>Wildfires Threaten Californiaâ€™s First Legal ...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "             tweet_id    text_info  text_info_conf  \\\n",
       "0  917791044158185473  informative          1.0000   \n",
       "1  917791130590183424  informative          1.0000   \n",
       "2  917791291823591425  informative          0.6813   \n",
       "3  917792092100988929  informative          0.6727   \n",
       "4  917792147700465664  informative          0.7143   \n",
       "\n",
       "                                          tweet_text  \n",
       "0  RT @Gizmodo: Wildfires raging through Northern...  \n",
       "1  PHOTOS: Deadly wildfires rage in California ht...  \n",
       "2  RT @Cal_OES: PLS SHARE: Weâ€™re capturing wild...  \n",
       "3  RT @TIME: California's raging wildfires as you...  \n",
       "4  Wildfires Threaten Californiaâ€™s First Legal ...  "
      ]
     },
     "execution_count": 111,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_text = pd.read_csv('/home/jacruz/crisis/data/crisis_texts_dataset.csv')\n",
    "df_text.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 112,
   "id": "60981829",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'does', \"doesn't\", 'further', 'him', 'very', 'i', \"you'll\", 'not', 'mightn', 'me', 'we', 'd', 'are', 'is', 'at', 'isn', 'too', 'between', 'hadn', 'doing', 'needn', 'ma', 'any', 'few', 'you', \"they've\", 'hasn', 'will', 'against', \"needn't\", 'while', \"you've\", 'on', 'ain', 'he', 'each', 'RT', \"mightn't\", 'by', 'yourselves', \"it's\", \"won't\", \"shouldn't\", 'such', 'own', 'as', 'both', \"i've\", 'm', 'o', 'this', 'myself', 'mustn', \"i'm\", 'his', 'hers', 'did', 'again', 'an', 'whom', 'itself', 'that', 'down', 'weren', 'off', 'haven', 'no', \"wouldn't\", \"i'd\", 'of', 'my', 'their', 'had', 'she', 'was', 'being', \"hasn't\", 'into', 'nor', 'now', \"haven't\", 'won', 'here', 'rt', 'ours', \"should've\", \"they'd\", \"he'd\", \"wasn't\", 'why', 'who', \"it'd\", 'with', 'our', 'more', \"she'll\", 'a', 'shouldn', \"weren't\", 'because', 'through', 'himself', \"that'll\", 'after', 'just', 'most', 'the', 'has', 'were', 'been', 'don', \"she'd\", 'when', 'until', \"couldn't\", 've', \"we'll\", 'once', 'ourselves', 'these', \"we'd\", 'all', \"don't\", 'didn', 'during', 'same', 'should', \"they'll\", \"it'll\", 'doesn', 'from', \"he'll\", 'above', \"aren't\", 'some', 'y', 'her', 'its', 'in', 'up', 'below', 'before', 'yourself', 'but', 'and', \"they're\", 'out', \"shan't\", 'they', 're', 'if', 'about', 'aren', 'there', 'wouldn', 'how', 'over', \"we're\", 'to', 't', 'can', 'having', \"mustn't\", \"isn't\", 'so', 'themselves', 'then', 'shan', 'where', \"didn't\", \"you'd\", 'have', 'those', \"you're\", \"she's\", 'what', 'only', 'which', \"i'll\", 'wasn', 'it', 'your', 'under', 'theirs', 'be', 'am', 'herself', \"he's\", 's', 'couldn', \"hadn't\", 'for', 'll', 'or', 'than', 'do', 'other', 'yours', \"we've\", 'them'}\n"
     ]
    }
   ],
   "source": [
    "# Para el manejo de stopwords\n",
    "\n",
    "#nltk.download('stopwords')\n",
    "stop_words = set(stopwords.words('english'))\n",
    "stop_words.update(['rt', 'RT'])\n",
    "\n",
    "print(stop_words)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 113,
   "id": "61dc97e1",
   "metadata": {},
   "outputs": [],
   "source": [
    "def clean_text(text):\n",
    "    # Eliminar URLs\n",
    "    text = re.sub(r'http\\S+|www\\S+|https\\S+', '', text, flags=re.MULTILINE)\n",
    "    # Eliminar caracteres no ASCII\n",
    "    text = text.encode('ascii', 'ignore').decode('ascii')\n",
    "    # Eliminar números\n",
    "    text = re.sub(r'\\d+', '', text)\n",
    "    # Reemplazar signos de puntuación con espacios\n",
    "    text = re.sub(r'[^\\w\\s]', ' ', text)\n",
    "    # Convertir a minúsculas y eliminar stopwords\n",
    "    text = ' '.join([word.lower() for word in text.split() if word.lower() not in stop_words])\n",
    "    return text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 114,
   "id": "c770afb9",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>tweet_id</th>\n",
       "      <th>text_info</th>\n",
       "      <th>text_info_conf</th>\n",
       "      <th>tweet_text</th>\n",
       "      <th>clean_text</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>917791044158185473</td>\n",
       "      <td>informative</td>\n",
       "      <td>1.0000</td>\n",
       "      <td>RT @Gizmodo: Wildfires raging through Northern...</td>\n",
       "      <td>gizmodo wildfires raging northern california t...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>917791130590183424</td>\n",
       "      <td>informative</td>\n",
       "      <td>1.0000</td>\n",
       "      <td>PHOTOS: Deadly wildfires rage in California ht...</td>\n",
       "      <td>photos deadly wildfires rage california</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>917791291823591425</td>\n",
       "      <td>informative</td>\n",
       "      <td>0.6813</td>\n",
       "      <td>RT @Cal_OES: PLS SHARE: Weâ€™re capturing wild...</td>\n",
       "      <td>cal_oes pls share capturing wildfire response ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>917792092100988929</td>\n",
       "      <td>informative</td>\n",
       "      <td>0.6727</td>\n",
       "      <td>RT @TIME: California's raging wildfires as you...</td>\n",
       "      <td>time california raging wildfires never seen</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>917792147700465664</td>\n",
       "      <td>informative</td>\n",
       "      <td>0.7143</td>\n",
       "      <td>Wildfires Threaten Californiaâ€™s First Legal ...</td>\n",
       "      <td>wildfires threaten californias first legal can...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>95</th>\n",
       "      <td>917833987824979968</td>\n",
       "      <td>informative</td>\n",
       "      <td>0.6783</td>\n",
       "      <td>Satellite image of California last night. Thos...</td>\n",
       "      <td>satellite image california last night arent li...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>96</th>\n",
       "      <td>917834588814155776</td>\n",
       "      <td>informative</td>\n",
       "      <td>0.6854</td>\n",
       "      <td>RT @AuroraWorldView: Deadly #California #wildf...</td>\n",
       "      <td>auroraworldview deadly california wildfires fo...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>97</th>\n",
       "      <td>917834628290920448</td>\n",
       "      <td>informative</td>\n",
       "      <td>0.6667</td>\n",
       "      <td>Fire swept through Northern California early M...</td>\n",
       "      <td>fire swept northern california early monday mo...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>98</th>\n",
       "      <td>917835067069788162</td>\n",
       "      <td>informative</td>\n",
       "      <td>1.0000</td>\n",
       "      <td>Couple, ages 100 and 98, die in CaliforniaÂ wi...</td>\n",
       "      <td>couple ages die california wildfire</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>99</th>\n",
       "      <td>917835123890049025</td>\n",
       "      <td>informative</td>\n",
       "      <td>0.6436</td>\n",
       "      <td>Napa Fire Map: Napa Valley Timeline, Death Tol...</td>\n",
       "      <td>napa fire map napa valley timeline death toll ...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>100 rows × 5 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "              tweet_id    text_info  text_info_conf  \\\n",
       "0   917791044158185473  informative          1.0000   \n",
       "1   917791130590183424  informative          1.0000   \n",
       "2   917791291823591425  informative          0.6813   \n",
       "3   917792092100988929  informative          0.6727   \n",
       "4   917792147700465664  informative          0.7143   \n",
       "..                 ...          ...             ...   \n",
       "95  917833987824979968  informative          0.6783   \n",
       "96  917834588814155776  informative          0.6854   \n",
       "97  917834628290920448  informative          0.6667   \n",
       "98  917835067069788162  informative          1.0000   \n",
       "99  917835123890049025  informative          0.6436   \n",
       "\n",
       "                                           tweet_text  \\\n",
       "0   RT @Gizmodo: Wildfires raging through Northern...   \n",
       "1   PHOTOS: Deadly wildfires rage in California ht...   \n",
       "2   RT @Cal_OES: PLS SHARE: Weâ€™re capturing wild...   \n",
       "3   RT @TIME: California's raging wildfires as you...   \n",
       "4   Wildfires Threaten Californiaâ€™s First Legal ...   \n",
       "..                                                ...   \n",
       "95  Satellite image of California last night. Thos...   \n",
       "96  RT @AuroraWorldView: Deadly #California #wildf...   \n",
       "97  Fire swept through Northern California early M...   \n",
       "98  Couple, ages 100 and 98, die in CaliforniaÂ wi...   \n",
       "99  Napa Fire Map: Napa Valley Timeline, Death Tol...   \n",
       "\n",
       "                                           clean_text  \n",
       "0   gizmodo wildfires raging northern california t...  \n",
       "1             photos deadly wildfires rage california  \n",
       "2   cal_oes pls share capturing wildfire response ...  \n",
       "3         time california raging wildfires never seen  \n",
       "4   wildfires threaten californias first legal can...  \n",
       "..                                                ...  \n",
       "95  satellite image california last night arent li...  \n",
       "96  auroraworldview deadly california wildfires fo...  \n",
       "97  fire swept northern california early monday mo...  \n",
       "98                couple ages die california wildfire  \n",
       "99  napa fire map napa valley timeline death toll ...  \n",
       "\n",
       "[100 rows x 5 columns]"
      ]
     },
     "execution_count": 114,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_text['clean_text'] = df_text['tweet_text'].apply(clean_text)\n",
    "df_text.head(100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 115,
   "id": "242a44f6",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['vatf yesterday eastern puerto rico red squad ks hurricanemaria rescuedogs',\n",
       " 'lebron cavs may sounding like jim mora soon hope win game nbafinals',\n",
       " 'gophers basketball pav benefit puerto rico',\n",
       " 'mommy dont cry year old girl surviving hurricane maria unicef connect',\n",
       " 'emily swick reports stuco efforts perrysburg help harvey victims',\n",
       " 'hurricane maria update forecaster shock warning us impact clear',\n",
       " 'proud pdalesd student council ss making difference hurricaneharvey relief achieves tigerready',\n",
       " 'hometown jayuya pr destroyed hurricane maria',\n",
       " 'early evening wednesday forecast track hurricane maria wsav wsavkrisa kyledenniswx wsavariellas',\n",
       " 'bid twofux makeup case auction benef glamberts affected hurricanes harvey irma']"
      ]
     },
     "execution_count": 115,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Muestra aleatoria de textos limpios\n",
    "random.sample(list(df_text['clean_text']), 10)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7f52560c",
   "metadata": {},
   "source": [
    "#### **Image**\n",
    "- Redimensionar y normalizar (224x224)\n",
    "- Valores de los pixeles entre 0 y 1\n",
    "- Normalizar los canales de color con respecto al dataset ImageNet"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 116,
   "id": "99d9dbf3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Los datos ya fueron preprocesados y guardados como archivos .npy\n",
    "# Falta cargar los datos\n",
    "default_path = \"/home/jacruz/crisis/data/preprocessed_images/\"\n",
    "df = pd.read_csv(\"/home/jacruz/crisis/data/crisis_images_dataset.csv\")\n",
    "\n",
    "image_names = df['image_name'].tolist()\n",
    "labels = df['image_info'].tolist()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 117,
   "id": "13d2cafd",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['/home/jacruz/crisis/data/preprocessed_images/917791044158185473_0.npy',\n",
       " '/home/jacruz/crisis/data/preprocessed_images/917791130590183424_0.npy',\n",
       " '/home/jacruz/crisis/data/preprocessed_images/917791291823591425_0.npy',\n",
       " '/home/jacruz/crisis/data/preprocessed_images/917791291823591425_1.npy',\n",
       " '/home/jacruz/crisis/data/preprocessed_images/917792092100988929_0.npy']"
      ]
     },
     "execution_count": 117,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "image_paths = [default_path + name + '.npy' for name in image_names]\n",
    "image_paths[:5]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 118,
   "id": "28a2509f",
   "metadata": {},
   "outputs": [],
   "source": [
    "image_path = image_paths[0]\n",
    "if not os.path.exists(image_path):\n",
    "                print(f\"Archivo no encontrado: {image_path}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 119,
   "id": "438f953f",
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_images_from_paths(image_paths, target_size=(224, 224)):\n",
    "    \"\"\"\n",
    "    Carga imágenes desde rutas de archivo\n",
    "    \"\"\"\n",
    "    images = []\n",
    "    valid_indices = []\n",
    "    \n",
    "    print(f\"Cargando {len(image_paths)} imágenes desde rutas...\")\n",
    "    \n",
    "    for i, npy_path in enumerate(image_paths):\n",
    "        try:\n",
    "            # Verificar si el archivo existe\n",
    "            if not os.path.exists(npy_path):\n",
    "                print(f\"Archivo no encontrado: {npy_path}\")\n",
    "                continue\n",
    "                \n",
    "            # Cargar array numpy\n",
    "            img_array = np.load(npy_path)\n",
    "            \n",
    "            images.append(img_array)\n",
    "            valid_indices.append(i)\n",
    "            \n",
    "        except Exception as e:\n",
    "            print(f\"Error cargando imagen {npy_path}: {e}\")\n",
    "            print(f\"Forma del array: {img_array.shape}\" if 'img_array' in locals() else \"No se pudo cargar el array.\")\n",
    "            continue\n",
    "            \n",
    "        # Mostrar progreso cada 1000 imágenes\n",
    "        if (i + 1) % 1000 == 0:\n",
    "            print(f\"Procesadas {i + 1}/{len(image_paths)} imágenes\")\n",
    "    \n",
    "    print(f\"Imágenes cargadas exitosamente: {len(images)}/{len(image_paths)}\")\n",
    "    \n",
    "    return np.array(images), valid_indices"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 120,
   "id": "ab1c9912",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def load_and_prepare_data():\n",
    "    \"\"\"\n",
    "    Carga los arrays numpy preprocesados (.npy) y prepara para entrenamiento\n",
    "    \"\"\"\n",
    "    # Cargar los arrays\n",
    "    X_paths = np.array(image_paths)\n",
    "    y_labels = np.array(labels)\n",
    "    \n",
    "    # Verificar formas\n",
    "    print(f\"Número de rutas de imágenes: {len(X_paths)}\")\n",
    "    print(f\"Número de etiquetas: {len(y_labels)}\")\n",
    "\n",
    "    # Cargar imágenes desde las rutas\n",
    "    X_images, valid_indices = load_images_from_paths(X_paths)\n",
    "\n",
    "    # Filtrar etiquetas para mantener solo las correspondientes a imágenes cargadas exitosamente\n",
    "    y_labels_filtered = y_labels[valid_indices]\n",
    "    \n",
    "    # Mapeo manual de labels a números\n",
    "    # Según el paper: 0 = Not-informative, 1 = Informative\n",
    "    label_mapping = {'not_informative': 0, 'informative': 1}\n",
    "    \n",
    "    # Convertir etiquetas de texto a numéricas\n",
    "    y_numeric = np.array([label_mapping[label] for label in y_labels_filtered])\n",
    "    \n",
    "    print(f\"Distribución de etiquetas:\")\n",
    "    print(f\"Non-informative (0): {np.sum(y_numeric == 0)}\")\n",
    "    print(f\"Informative (1): {np.sum(y_numeric == 1)}\")\n",
    "\n",
    "    # Se asegura de tener el formato correcto\n",
    "    X_images = X_images.astype('float32')\n",
    "\n",
    "    y_categorical = to_categorical(y_numeric, num_classes=2)\n",
    "\n",
    "    print(f\"Rango de valores en X: min {X_images.min():.3f}, max {X_images.max():.3f}\")\n",
    "\n",
    "    print(f\"Forma final de X: {X_images.shape}\")\n",
    "    print(f\"Forma final de y: {y_categorical.shape}\")\n",
    "    return X_images, y_categorical"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 121,
   "id": "e9dc40ff",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Imágenes cargadas: 18082\n"
     ]
    }
   ],
   "source": [
    "# Cargar dataset de imágenes\n",
    "df_image = pd.read_csv('/home/jacruz/crisis/data/crisis_images_dataset.csv')\n",
    "print(f\"Imágenes cargadas: {len(df_image)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fdcef4fc",
   "metadata": {},
   "source": [
    "#### **Multimodal**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 122,
   "id": "26c0b6f9",
   "metadata": {},
   "outputs": [],
   "source": [
    "def align_multimodal_pairs_paper(df_text, df_image, image_base_path):\n",
    "    \"\"\"\n",
    "    Alinea pares texto-imagen con misma etiqueta\n",
    "    \"\"\"\n",
    "    # Asegurar que tenemos las columnas necesarias\n",
    "    print(\"Alineando pares texto-imagen con misma etiqueta...\")\n",
    "    \n",
    "    aligned_pairs = []\n",
    "    \n",
    "    # Contadores para debugging\n",
    "    total_texts = len(df_text)\n",
    "    matches_found = 0\n",
    "    \n",
    "    for idx, text_row in df_text.iterrows():\n",
    "        tweet_id = text_row['tweet_id']\n",
    "        text_label = text_row['text_info']  # 'informative' o 'not_informative'\n",
    "        clean_text = text_row['clean_text']\n",
    "        \n",
    "        # Buscar imagen con mismo tweet_id y MISMA etiqueta\n",
    "        matching_images = df_image[\n",
    "            (df_image['tweet_id'] == tweet_id) & \n",
    "            (df_image['image_info'] == text_label)\n",
    "        ]\n",
    "        \n",
    "        if len(matching_images) > 0:\n",
    "            # Tomar la primera imagen que coincide\n",
    "            image_row = matching_images.iloc[0]\n",
    "            image_name = image_row['image_name']\n",
    "            image_path = f\"{image_base_path}/{image_name}.npy\"\n",
    "            \n",
    "            # Verificar que el archivo de imagen existe\n",
    "            if os.path.exists(image_path):\n",
    "                aligned_pairs.append({\n",
    "                    'tweet_id': tweet_id,\n",
    "                    'text': clean_text,\n",
    "                    'image_path': image_path,\n",
    "                    'label': text_label,\n",
    "                    'label_numeric': 1 if text_label == 'informative' else 0\n",
    "                })\n",
    "                matches_found += 1\n",
    "        \n",
    "        # Mostrar progreso\n",
    "        if (idx + 1) % 1000 == 0:\n",
    "            print(f\"Procesados {idx + 1}/{total_texts} textos, encontrados {matches_found} pares\")\n",
    "    \n",
    "    print(f\"Pares alineados encontrados: {matches_found}/{total_texts}\")\n",
    "    \n",
    "    # Convertir a DataFrame\n",
    "    aligned_df = pd.DataFrame(aligned_pairs)\n",
    "    \n",
    "    # Mostrar distribución\n",
    "    if len(aligned_df) > 0:\n",
    "        informative_count = sum(aligned_df['label_numeric'] == 1)\n",
    "        non_informative_count = sum(aligned_df['label_numeric'] == 0)\n",
    "        \n",
    "        print(f\"Distribución de pares alineados:\")\n",
    "        print(f\"  Informative: {informative_count} ({informative_count/len(aligned_df)*100:.1f}%)\")\n",
    "        print(f\"  Non-informative: {non_informative_count} ({non_informative_count/len(aligned_df)*100:.1f}%)\")\n",
    "    else:\n",
    "        print(\"⚠️  No se encontraron pares alineados. Revisa los datos.\")\n",
    "    \n",
    "    return aligned_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 123,
   "id": "353740c0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Alineando pares texto-imagen con misma etiqueta...\n",
      "Procesados 1000/16058 textos, encontrados 783 pares\n",
      "Procesados 2000/16058 textos, encontrados 1547 pares\n",
      "Procesados 3000/16058 textos, encontrados 2380 pares\n",
      "Procesados 4000/16058 textos, encontrados 3080 pares\n",
      "Procesados 5000/16058 textos, encontrados 3793 pares\n",
      "Procesados 6000/16058 textos, encontrados 4499 pares\n",
      "Procesados 7000/16058 textos, encontrados 5199 pares\n",
      "Procesados 8000/16058 textos, encontrados 5829 pares\n",
      "Procesados 9000/16058 textos, encontrados 6428 pares\n",
      "Procesados 10000/16058 textos, encontrados 7042 pares\n",
      "Procesados 11000/16058 textos, encontrados 7752 pares\n",
      "Procesados 12000/16058 textos, encontrados 8443 pares\n",
      "Procesados 13000/16058 textos, encontrados 9103 pares\n",
      "Procesados 14000/16058 textos, encontrados 9750 pares\n",
      "Procesados 15000/16058 textos, encontrados 10540 pares\n",
      "Procesados 16000/16058 textos, encontrados 11354 pares\n",
      "Pares alineados encontrados: 11398/16058\n",
      "Distribución de pares alineados:\n",
      "  Informative: 7624 (66.9%)\n",
      "  Non-informative: 3774 (33.1%)\n"
     ]
    }
   ],
   "source": [
    "# Alinear pares\n",
    "aligned_df = align_multimodal_pairs_paper(\n",
    "    df_text, \n",
    "    df_image, \n",
    "    image_base_path=\"/home/jacruz/crisis/data/preprocessed_images\"\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 124,
   "id": "2b100077",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>tweet_id</th>\n",
       "      <th>text</th>\n",
       "      <th>image_path</th>\n",
       "      <th>label</th>\n",
       "      <th>label_numeric</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>917791044158185473</td>\n",
       "      <td>gizmodo wildfires raging northern california t...</td>\n",
       "      <td>/home/jacruz/crisis/data/preprocessed_images/9...</td>\n",
       "      <td>informative</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>917791130590183424</td>\n",
       "      <td>photos deadly wildfires rage california</td>\n",
       "      <td>/home/jacruz/crisis/data/preprocessed_images/9...</td>\n",
       "      <td>informative</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>917791291823591425</td>\n",
       "      <td>cal_oes pls share capturing wildfire response ...</td>\n",
       "      <td>/home/jacruz/crisis/data/preprocessed_images/9...</td>\n",
       "      <td>informative</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>917792092100988929</td>\n",
       "      <td>time california raging wildfires never seen</td>\n",
       "      <td>/home/jacruz/crisis/data/preprocessed_images/9...</td>\n",
       "      <td>informative</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>917792147700465664</td>\n",
       "      <td>wildfires threaten californias first legal can...</td>\n",
       "      <td>/home/jacruz/crisis/data/preprocessed_images/9...</td>\n",
       "      <td>informative</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11393</th>\n",
       "      <td>916027579882029056</td>\n",
       "      <td>sun earthquake model matches mexico</td>\n",
       "      <td>/home/jacruz/crisis/data/preprocessed_images/9...</td>\n",
       "      <td>informative</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11394</th>\n",
       "      <td>916075589131436032</td>\n",
       "      <td>wave natural disasters strike mexico</td>\n",
       "      <td>/home/jacruz/crisis/data/preprocessed_images/9...</td>\n",
       "      <td>informative</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11395</th>\n",
       "      <td>916099461444710400</td>\n",
       "      <td>podcast shines light volunteers rushed help me...</td>\n",
       "      <td>/home/jacruz/crisis/data/preprocessed_images/9...</td>\n",
       "      <td>informative</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11396</th>\n",
       "      <td>916112796194021376</td>\n",
       "      <td>entercom san francisco stations raise funds me...</td>\n",
       "      <td>/home/jacruz/crisis/data/preprocessed_images/9...</td>\n",
       "      <td>informative</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11397</th>\n",
       "      <td>916164076484653056</td>\n",
       "      <td>mexico earthquakes international medical corps...</td>\n",
       "      <td>/home/jacruz/crisis/data/preprocessed_images/9...</td>\n",
       "      <td>informative</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>11398 rows × 5 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                 tweet_id                                               text  \\\n",
       "0      917791044158185473  gizmodo wildfires raging northern california t...   \n",
       "1      917791130590183424            photos deadly wildfires rage california   \n",
       "2      917791291823591425  cal_oes pls share capturing wildfire response ...   \n",
       "3      917792092100988929        time california raging wildfires never seen   \n",
       "4      917792147700465664  wildfires threaten californias first legal can...   \n",
       "...                   ...                                                ...   \n",
       "11393  916027579882029056                sun earthquake model matches mexico   \n",
       "11394  916075589131436032               wave natural disasters strike mexico   \n",
       "11395  916099461444710400  podcast shines light volunteers rushed help me...   \n",
       "11396  916112796194021376  entercom san francisco stations raise funds me...   \n",
       "11397  916164076484653056  mexico earthquakes international medical corps...   \n",
       "\n",
       "                                              image_path        label  \\\n",
       "0      /home/jacruz/crisis/data/preprocessed_images/9...  informative   \n",
       "1      /home/jacruz/crisis/data/preprocessed_images/9...  informative   \n",
       "2      /home/jacruz/crisis/data/preprocessed_images/9...  informative   \n",
       "3      /home/jacruz/crisis/data/preprocessed_images/9...  informative   \n",
       "4      /home/jacruz/crisis/data/preprocessed_images/9...  informative   \n",
       "...                                                  ...          ...   \n",
       "11393  /home/jacruz/crisis/data/preprocessed_images/9...  informative   \n",
       "11394  /home/jacruz/crisis/data/preprocessed_images/9...  informative   \n",
       "11395  /home/jacruz/crisis/data/preprocessed_images/9...  informative   \n",
       "11396  /home/jacruz/crisis/data/preprocessed_images/9...  informative   \n",
       "11397  /home/jacruz/crisis/data/preprocessed_images/9...  informative   \n",
       "\n",
       "       label_numeric  \n",
       "0                  1  \n",
       "1                  1  \n",
       "2                  1  \n",
       "3                  1  \n",
       "4                  1  \n",
       "...              ...  \n",
       "11393              1  \n",
       "11394              1  \n",
       "11395              1  \n",
       "11396              1  \n",
       "11397              1  \n",
       "\n",
       "[11398 rows x 5 columns]"
      ]
     },
     "execution_count": 124,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "aligned_df"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e07e6363",
   "metadata": {},
   "source": [
    "#### **Split de Datos**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 125,
   "id": "dc2bc625",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "from tensorflow.keras.preprocessing.text import Tokenizer\n",
    "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
    "\n",
    "def create_multimodal_splits_paper(aligned_df, test_size=0.15, val_size=0.15, max_seq_len=50):\n",
    "    \"\"\"\n",
    "    Crea splits 70-15-15\n",
    "    \"\"\"\n",
    "    if len(aligned_df) == 0:\n",
    "        raise ValueError(\"No hay datos alineados para crear splits\")\n",
    "    \n",
    "    print(\"Creando splits...\")\n",
    "    \n",
    "    # 1. Split estratificado (70% train, 30% temp)\n",
    "    train_df, temp_df = train_test_split(\n",
    "        aligned_df, \n",
    "        test_size=test_size + val_size, \n",
    "        random_state=42,\n",
    "        stratify=aligned_df['label_numeric']\n",
    "    )\n",
    "    \n",
    "    # 2. Split temp en validation y test (15% cada uno)\n",
    "    val_df, test_df = train_test_split(\n",
    "        temp_df,\n",
    "        test_size=0.5,  # 15%/(15%+15%) = 0.5\n",
    "        random_state=42,\n",
    "        stratify=temp_df['label_numeric']\n",
    "    )\n",
    "    \n",
    "    print(f\"Split final (70-15-15):\")\n",
    "    print(f\"  Train: {len(train_df)} samples\")\n",
    "    train_informative = sum(train_df['label_numeric'] == 1)\n",
    "    train_non_informative = sum(train_df['label_numeric'] == 0)\n",
    "    print(f\"    Informative: {train_informative} ({train_informative/len(train_df)*100:.1f}%)\")\n",
    "    print(f\"    Non-informative: {train_non_informative} ({train_non_informative/len(train_df)*100:.1f}%)\")\n",
    "\n",
    "    val_informative = sum(val_df['label_numeric'] == 1)\n",
    "    val_non_informative = sum(val_df['label_numeric'] == 0)\n",
    "    print(f\"  Validation: {len(val_df)} samples\")\n",
    "    print(f\"    Informative: {val_informative} ({val_informative/len(val_df)*100:.1f}%)\")\n",
    "    print(f\"    Non-informative: {val_non_informative} ({val_non_informative/len(val_df)*100:.1f}%)\")\n",
    "    \n",
    "    test_informative = sum(test_df['label_numeric'] == 1)\n",
    "    test_non_informative = sum(test_df['label_numeric'] == 0)\n",
    "    print(f\"  Test: {len(test_df)} samples\")\n",
    "    print(f\"    Informative: {test_informative} ({test_informative/len(test_df)*100:.1f}%)\")\n",
    "    print(f\"    Non-informative: {test_non_informative} ({test_non_informative/len(test_df)*100:.1f}%)\")\n",
    "    \n",
    "    # 3. Tokenización de textos\n",
    "    print(\"Tokenizando textos...\")\n",
    "    tokenizer = Tokenizer(num_words=50000, oov_token=\"<OOV>\")\n",
    "    tokenizer.fit_on_texts(train_df['text'])\n",
    "    \n",
    "    # Convertir textos a secuencias\n",
    "    X_text_train = tokenizer.texts_to_sequences(train_df['text'])\n",
    "    X_text_val = tokenizer.texts_to_sequences(val_df['text'])\n",
    "    X_text_test = tokenizer.texts_to_sequences(test_df['text'])\n",
    "    \n",
    "    # Padding (max_seq_len=50 como en tu implementación)\n",
    "    X_text_train = pad_sequences(X_text_train, maxlen=max_seq_len, padding='post')\n",
    "    X_text_val = pad_sequences(X_text_val, maxlen=max_seq_len, padding='post')\n",
    "    X_text_test = pad_sequences(X_text_test, maxlen=max_seq_len, padding='post')\n",
    "    \n",
    "    # 4. Cargar imágenes\n",
    "    print(\"Cargando imágenes...\")\n",
    "    X_image_train, train_img_indices = load_images_from_paths(train_df['image_path'].tolist())\n",
    "    X_image_val, val_img_indices = load_images_from_paths(val_df['image_path'].tolist())\n",
    "    X_image_test, test_img_indices = load_images_from_paths(test_df['image_path'].tolist())\n",
    "    \n",
    "    # 5. Filtrar textos para coincidir con imágenes cargadas exitosamente\n",
    "    X_text_train = X_text_train[train_img_indices]\n",
    "    X_text_val = X_text_val[val_img_indices]\n",
    "    X_text_test = X_text_test[test_img_indices]\n",
    "    \n",
    "    # 6. Filtrar labels\n",
    "    y_train = train_df['label_numeric'].values[train_img_indices]\n",
    "    y_val = val_df['label_numeric'].values[val_img_indices]\n",
    "    y_test = test_df['label_numeric'].values[test_img_indices]\n",
    "    \n",
    "    print(f\"\\nDatasets finales multimodales:\")\n",
    "    print(f\"  X_text_train: {X_text_train.shape}, X_image_train: {X_image_train.shape}, y_train: {y_train.shape}\")\n",
    "    print(f\"  X_text_val: {X_text_val.shape}, X_image_val: {X_image_val.shape}, y_val: {y_val.shape}\")\n",
    "    print(f\"  X_text_test: {X_text_test.shape}, X_image_test: {X_image_test.shape}, y_test: {y_test.shape}\")\n",
    "    \n",
    "    return (X_text_train, X_image_train, y_train,\n",
    "            X_text_val, X_image_val, y_val, \n",
    "            X_text_test, X_image_test, y_test, tokenizer)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 126,
   "id": "6903885e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Creando splits...\n",
      "Split final (70-15-15):\n",
      "  Train: 7978 samples\n",
      "    Informative: 5336 (66.9%)\n",
      "    Non-informative: 2642 (33.1%)\n",
      "  Validation: 1710 samples\n",
      "    Informative: 1144 (66.9%)\n",
      "    Non-informative: 566 (33.1%)\n",
      "  Test: 1710 samples\n",
      "    Informative: 1144 (66.9%)\n",
      "    Non-informative: 566 (33.1%)\n",
      "Tokenizando textos...\n",
      "Cargando imágenes...\n",
      "Cargando 7978 imágenes desde rutas...\n",
      "Procesadas 1000/7978 imágenes\n",
      "Procesadas 2000/7978 imágenes\n",
      "Procesadas 3000/7978 imágenes\n",
      "Procesadas 4000/7978 imágenes\n",
      "Procesadas 5000/7978 imágenes\n",
      "Procesadas 6000/7978 imágenes\n",
      "Procesadas 7000/7978 imágenes\n",
      "Imágenes cargadas exitosamente: 7978/7978\n",
      "Cargando 1710 imágenes desde rutas...\n",
      "Procesadas 1000/1710 imágenes\n",
      "Imágenes cargadas exitosamente: 1710/1710\n",
      "Cargando 1710 imágenes desde rutas...\n",
      "Procesadas 1000/1710 imágenes\n",
      "Imágenes cargadas exitosamente: 1710/1710\n",
      "\n",
      "Datasets finales multimodales:\n",
      "  X_text_train: (7978, 50), X_image_train: (7978, 224, 224, 3), y_train: (7978,)\n",
      "  X_text_val: (1710, 50), X_image_val: (1710, 224, 224, 3), y_val: (1710,)\n",
      "  X_text_test: (1710, 50), X_image_test: (1710, 224, 224, 3), y_test: (1710,)\n"
     ]
    }
   ],
   "source": [
    "# Crear splits multimodales\n",
    "(X_text_train, X_image_train, y_train,\n",
    " X_text_val, X_image_val, y_val,\n",
    " X_text_test, X_image_test, y_test, tokenizer) = create_multimodal_splits_paper(aligned_df)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bf5450a2",
   "metadata": {},
   "source": [
    "### **Modelo Word2Vec**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "af7a41c1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Jugando con Word2Vec\n",
    "sentences = X_text_train['text'].tolist()  # Se pasa a lista de listas de tokens\n",
    "w2v = Word2Vec(sentences, vector_size=300, window=5, min_count=5, workers=4)\n",
    "\n",
    "vocab = list(w2v.wv.key_to_index.keys())\n",
    "\n",
    "palabra_a_buscar = 'mexico'\n",
    "print(f\"El vocabulario tiene {len(vocab)} palabras\")\n",
    "print(f\"Algunas palabras aleatorias del vocabulario: {sample(vocab, 10)}\")\n",
    "\n",
    "# Buscar palabras similares\n",
    "w2v.wv.most_similar(palabra_a_buscar)\n",
    "print(f\"Palabras más similares a '{palabra_a_buscar}': {w2v.wv.most_similar(palabra_a_buscar)}\")\n",
    "\n",
    "# Similitud entre dos palabras\n",
    "palabra1 = 'help'\n",
    "palabra2 = 'support'\n",
    "similarity = w2v.wv.similarity(palabra1, palabra2)\n",
    "print(f\"La similitud entre '{palabra1}' y '{palabra2}' es: {similarity}\")\n",
    "\n",
    "def create_embedding_matrix(word_index, w2v_model, embedding_dim=300):\n",
    "    vocab_size = len(word_index) + 1\n",
    "    # matriz de embedding inicializada en ceros\n",
    "    embedding_matrix = np.zeros((vocab_size, embedding_dim))\n",
    "    for word, i in word_index.items():\n",
    "        if word in w2v_model.wv:\n",
    "            # si la palabra está en el vocabulario de word2vec\n",
    "            embedding_matrix[i] = w2v_model.wv[word]\n",
    "        else:\n",
    "            # si la palabra no está, vector aleatorio\n",
    "            embedding_matrix[i] = np.random.normal(0, 0.1, embedding_dim)\n",
    "    return embedding_matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0dc2fe2c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Cargando embeddings GloVe...\n",
      "Se cargaron 400000 vectores de palabras.\n",
      "Palabras encontradas en GloVe: 10470/14333\n",
      "Palabras no encontradas: 3862\n"
     ]
    }
   ],
   "source": [
    "# Crear matriz de embedding\n",
    "vocab_size = min(len(tokenizer.word_index) + 1, 50000)\n",
    "embedding_matrix = create_embedding_matrix(tokenizer.word_index, w2v, embedding_dim=300)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "84805fd9",
   "metadata": {},
   "source": [
    "### **Arquitectura Multimodal**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 129,
   "id": "690a3e14",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Creando arquitectura multimodal exacta del paper...\n",
      "✅ Modelo multimodal creado (configuración exacta paper)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/jacruz/.conda/envs/crisis-env/lib/python3.13/site-packages/keras/src/layers/core/embedding.py:97: UserWarning: Argument `input_length` is deprecated. Just remove it.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\">Model: \"multimodal_paper_exact\"</span>\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1mModel: \"multimodal_paper_exact\"\u001b[0m\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">┏━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━┓\n",
       "┃<span style=\"font-weight: bold\"> Layer (type)        </span>┃<span style=\"font-weight: bold\"> Output Shape      </span>┃<span style=\"font-weight: bold\">    Param # </span>┃<span style=\"font-weight: bold\"> Connected to      </span>┃\n",
       "┡━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━┩\n",
       "│ text_input          │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">50</span>)        │          <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │ -                 │\n",
       "│ (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">InputLayer</span>)        │                   │            │                   │\n",
       "├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n",
       "│ embedding           │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">50</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">300</span>)   │  <span style=\"color: #00af00; text-decoration-color: #00af00\">4,299,900</span> │ text_input[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>][<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>]  │\n",
       "│ (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Embedding</span>)         │                   │            │                   │\n",
       "├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n",
       "│ conv1d (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Conv1D</span>)     │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">50</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">100</span>)   │     <span style=\"color: #00af00; text-decoration-color: #00af00\">60,100</span> │ embedding[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>][<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>]   │\n",
       "├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n",
       "│ conv1d_1 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Conv1D</span>)   │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">50</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">150</span>)   │    <span style=\"color: #00af00; text-decoration-color: #00af00\">135,150</span> │ embedding[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>][<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>]   │\n",
       "├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n",
       "│ conv1d_2 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Conv1D</span>)   │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">50</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">200</span>)   │    <span style=\"color: #00af00; text-decoration-color: #00af00\">240,200</span> │ embedding[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>][<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>]   │\n",
       "├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n",
       "│ global_max_pooling… │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">100</span>)       │          <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │ conv1d[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>][<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>]      │\n",
       "│ (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">GlobalMaxPooling1…</span> │                   │            │                   │\n",
       "├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n",
       "│ global_max_pooling… │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">150</span>)       │          <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │ conv1d_1[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>][<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>]    │\n",
       "│ (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">GlobalMaxPooling1…</span> │                   │            │                   │\n",
       "├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n",
       "│ global_max_pooling… │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">200</span>)       │          <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │ conv1d_2[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>][<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>]    │\n",
       "│ (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">GlobalMaxPooling1…</span> │                   │            │                   │\n",
       "├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n",
       "│ image_input         │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">224</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">224</span>,  │          <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │ -                 │\n",
       "│ (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">InputLayer</span>)        │ <span style=\"color: #00af00; text-decoration-color: #00af00\">3</span>)                │            │                   │\n",
       "├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n",
       "│ concatenate         │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">450</span>)       │          <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │ global_max_pooli… │\n",
       "│ (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Concatenate</span>)       │                   │            │ global_max_pooli… │\n",
       "│                     │                   │            │ global_max_pooli… │\n",
       "├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n",
       "│ vgg16 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Functional</span>)  │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">7</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">7</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">512</span>) │ <span style=\"color: #00af00; text-decoration-color: #00af00\">14,714,688</span> │ image_input[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>][<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>] │\n",
       "├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n",
       "│ dropout (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dropout</span>)   │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">450</span>)       │          <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │ concatenate[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>][<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>] │\n",
       "├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n",
       "│ global_average_poo… │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">512</span>)       │          <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │ vgg16[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>][<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>]       │\n",
       "│ (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">GlobalAveragePool…</span> │                   │            │                   │\n",
       "├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n",
       "│ text_features       │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">1000</span>)      │    <span style=\"color: #00af00; text-decoration-color: #00af00\">451,000</span> │ dropout[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>][<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>]     │\n",
       "│ (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dense</span>)             │                   │            │                   │\n",
       "├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n",
       "│ image_features      │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">1000</span>)      │    <span style=\"color: #00af00; text-decoration-color: #00af00\">513,000</span> │ global_average_p… │\n",
       "│ (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dense</span>)             │                   │            │                   │\n",
       "├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n",
       "│ fusion              │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">2000</span>)      │          <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │ text_features[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>]… │\n",
       "│ (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Concatenate</span>)       │                   │            │ image_features[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>… │\n",
       "├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n",
       "│ dense (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dense</span>)       │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">512</span>)       │  <span style=\"color: #00af00; text-decoration-color: #00af00\">1,024,512</span> │ fusion[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>][<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>]      │\n",
       "├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n",
       "│ dropout_1 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dropout</span>) │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">512</span>)       │          <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │ dense[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>][<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>]       │\n",
       "├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n",
       "│ output (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dense</span>)      │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">2</span>)         │      <span style=\"color: #00af00; text-decoration-color: #00af00\">1,026</span> │ dropout_1[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>][<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>]   │\n",
       "└─────────────────────┴───────────────────┴────────────┴───────────────────┘\n",
       "</pre>\n"
      ],
      "text/plain": [
       "┏━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━┓\n",
       "┃\u001b[1m \u001b[0m\u001b[1mLayer (type)       \u001b[0m\u001b[1m \u001b[0m┃\u001b[1m \u001b[0m\u001b[1mOutput Shape     \u001b[0m\u001b[1m \u001b[0m┃\u001b[1m \u001b[0m\u001b[1m   Param #\u001b[0m\u001b[1m \u001b[0m┃\u001b[1m \u001b[0m\u001b[1mConnected to     \u001b[0m\u001b[1m \u001b[0m┃\n",
       "┡━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━┩\n",
       "│ text_input          │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m50\u001b[0m)        │          \u001b[38;5;34m0\u001b[0m │ -                 │\n",
       "│ (\u001b[38;5;33mInputLayer\u001b[0m)        │                   │            │                   │\n",
       "├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n",
       "│ embedding           │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m50\u001b[0m, \u001b[38;5;34m300\u001b[0m)   │  \u001b[38;5;34m4,299,900\u001b[0m │ text_input[\u001b[38;5;34m0\u001b[0m][\u001b[38;5;34m0\u001b[0m]  │\n",
       "│ (\u001b[38;5;33mEmbedding\u001b[0m)         │                   │            │                   │\n",
       "├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n",
       "│ conv1d (\u001b[38;5;33mConv1D\u001b[0m)     │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m50\u001b[0m, \u001b[38;5;34m100\u001b[0m)   │     \u001b[38;5;34m60,100\u001b[0m │ embedding[\u001b[38;5;34m0\u001b[0m][\u001b[38;5;34m0\u001b[0m]   │\n",
       "├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n",
       "│ conv1d_1 (\u001b[38;5;33mConv1D\u001b[0m)   │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m50\u001b[0m, \u001b[38;5;34m150\u001b[0m)   │    \u001b[38;5;34m135,150\u001b[0m │ embedding[\u001b[38;5;34m0\u001b[0m][\u001b[38;5;34m0\u001b[0m]   │\n",
       "├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n",
       "│ conv1d_2 (\u001b[38;5;33mConv1D\u001b[0m)   │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m50\u001b[0m, \u001b[38;5;34m200\u001b[0m)   │    \u001b[38;5;34m240,200\u001b[0m │ embedding[\u001b[38;5;34m0\u001b[0m][\u001b[38;5;34m0\u001b[0m]   │\n",
       "├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n",
       "│ global_max_pooling… │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m100\u001b[0m)       │          \u001b[38;5;34m0\u001b[0m │ conv1d[\u001b[38;5;34m0\u001b[0m][\u001b[38;5;34m0\u001b[0m]      │\n",
       "│ (\u001b[38;5;33mGlobalMaxPooling1…\u001b[0m │                   │            │                   │\n",
       "├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n",
       "│ global_max_pooling… │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m150\u001b[0m)       │          \u001b[38;5;34m0\u001b[0m │ conv1d_1[\u001b[38;5;34m0\u001b[0m][\u001b[38;5;34m0\u001b[0m]    │\n",
       "│ (\u001b[38;5;33mGlobalMaxPooling1…\u001b[0m │                   │            │                   │\n",
       "├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n",
       "│ global_max_pooling… │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m200\u001b[0m)       │          \u001b[38;5;34m0\u001b[0m │ conv1d_2[\u001b[38;5;34m0\u001b[0m][\u001b[38;5;34m0\u001b[0m]    │\n",
       "│ (\u001b[38;5;33mGlobalMaxPooling1…\u001b[0m │                   │            │                   │\n",
       "├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n",
       "│ image_input         │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m224\u001b[0m, \u001b[38;5;34m224\u001b[0m,  │          \u001b[38;5;34m0\u001b[0m │ -                 │\n",
       "│ (\u001b[38;5;33mInputLayer\u001b[0m)        │ \u001b[38;5;34m3\u001b[0m)                │            │                   │\n",
       "├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n",
       "│ concatenate         │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m450\u001b[0m)       │          \u001b[38;5;34m0\u001b[0m │ global_max_pooli… │\n",
       "│ (\u001b[38;5;33mConcatenate\u001b[0m)       │                   │            │ global_max_pooli… │\n",
       "│                     │                   │            │ global_max_pooli… │\n",
       "├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n",
       "│ vgg16 (\u001b[38;5;33mFunctional\u001b[0m)  │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m7\u001b[0m, \u001b[38;5;34m7\u001b[0m, \u001b[38;5;34m512\u001b[0m) │ \u001b[38;5;34m14,714,688\u001b[0m │ image_input[\u001b[38;5;34m0\u001b[0m][\u001b[38;5;34m0\u001b[0m] │\n",
       "├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n",
       "│ dropout (\u001b[38;5;33mDropout\u001b[0m)   │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m450\u001b[0m)       │          \u001b[38;5;34m0\u001b[0m │ concatenate[\u001b[38;5;34m0\u001b[0m][\u001b[38;5;34m0\u001b[0m] │\n",
       "├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n",
       "│ global_average_poo… │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m512\u001b[0m)       │          \u001b[38;5;34m0\u001b[0m │ vgg16[\u001b[38;5;34m0\u001b[0m][\u001b[38;5;34m0\u001b[0m]       │\n",
       "│ (\u001b[38;5;33mGlobalAveragePool…\u001b[0m │                   │            │                   │\n",
       "├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n",
       "│ text_features       │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m1000\u001b[0m)      │    \u001b[38;5;34m451,000\u001b[0m │ dropout[\u001b[38;5;34m0\u001b[0m][\u001b[38;5;34m0\u001b[0m]     │\n",
       "│ (\u001b[38;5;33mDense\u001b[0m)             │                   │            │                   │\n",
       "├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n",
       "│ image_features      │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m1000\u001b[0m)      │    \u001b[38;5;34m513,000\u001b[0m │ global_average_p… │\n",
       "│ (\u001b[38;5;33mDense\u001b[0m)             │                   │            │                   │\n",
       "├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n",
       "│ fusion              │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m2000\u001b[0m)      │          \u001b[38;5;34m0\u001b[0m │ text_features[\u001b[38;5;34m0\u001b[0m]… │\n",
       "│ (\u001b[38;5;33mConcatenate\u001b[0m)       │                   │            │ image_features[\u001b[38;5;34m0\u001b[0m… │\n",
       "├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n",
       "│ dense (\u001b[38;5;33mDense\u001b[0m)       │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m512\u001b[0m)       │  \u001b[38;5;34m1,024,512\u001b[0m │ fusion[\u001b[38;5;34m0\u001b[0m][\u001b[38;5;34m0\u001b[0m]      │\n",
       "├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n",
       "│ dropout_1 (\u001b[38;5;33mDropout\u001b[0m) │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m512\u001b[0m)       │          \u001b[38;5;34m0\u001b[0m │ dense[\u001b[38;5;34m0\u001b[0m][\u001b[38;5;34m0\u001b[0m]       │\n",
       "├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n",
       "│ output (\u001b[38;5;33mDense\u001b[0m)      │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m2\u001b[0m)         │      \u001b[38;5;34m1,026\u001b[0m │ dropout_1[\u001b[38;5;34m0\u001b[0m][\u001b[38;5;34m0\u001b[0m]   │\n",
       "└─────────────────────┴───────────────────┴────────────┴───────────────────┘\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Total params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">21,439,576</span> (81.79 MB)\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1m Total params: \u001b[0m\u001b[38;5;34m21,439,576\u001b[0m (81.79 MB)\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Trainable params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">17,139,676</span> (65.38 MB)\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1m Trainable params: \u001b[0m\u001b[38;5;34m17,139,676\u001b[0m (65.38 MB)\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Non-trainable params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">4,299,900</span> (16.40 MB)\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1m Non-trainable params: \u001b[0m\u001b[38;5;34m4,299,900\u001b[0m (16.40 MB)\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "from tensorflow.keras import layers, models\n",
    "from tensorflow.keras.applications import VGG16\n",
    "\n",
    "def create_multimodal_paper_exact(vocab_size, embedding_matrix, embedding_dim=300, max_seq_len=50):\n",
    "    \"\"\"\n",
    "    Implementación EXACTA del paper - entrenamiento desde cero\n",
    "    \"\"\"\n",
    "    print(\"Creando arquitectura multimodal exacta del paper...\")\n",
    "\n",
    "    # Usar mixed precision para ahorrar memoria\n",
    "    policy = tf.keras.mixed_precision.Policy('mixed_float16')\n",
    "    tf.keras.mixed_precision.set_global_policy(policy)\n",
    "\n",
    "    with tf.device('/GPU:0'):\n",
    "        # ==================== RAMA TEXTO ====================\n",
    "        text_input = layers.Input(shape=(max_seq_len,), name='text_input')\n",
    "        \n",
    "        # Embedding layer (igual que paper)\n",
    "        x_text = layers.Embedding(\n",
    "            input_dim=vocab_size,\n",
    "            output_dim=embedding_dim,\n",
    "            weights=[embedding_matrix],\n",
    "            input_length=max_seq_len,\n",
    "            trainable=False  # Como en el paper - embeddings congelados\n",
    "        )(text_input)\n",
    "        \n",
    "        # CNN con múltiples filtros (EXACTO como paper)\n",
    "        conv_blocks = []\n",
    "        \n",
    "        # Filter 1: 100 filters, size 2\n",
    "        conv1 = layers.Conv1D(filters=100, kernel_size=2, activation='relu', padding='same')(x_text)\n",
    "        pool1 = layers.GlobalMaxPooling1D()(conv1)\n",
    "        conv_blocks.append(pool1)\n",
    "        \n",
    "        # Filter 2: 150 filters, size 3  \n",
    "        conv2 = layers.Conv1D(filters=150, kernel_size=3, activation='relu', padding='same')(x_text)\n",
    "        pool2 = layers.GlobalMaxPooling1D()(conv2)\n",
    "        conv_blocks.append(pool2)\n",
    "        \n",
    "        # Filter 3: 200 filters, size 4\n",
    "        conv3 = layers.Conv1D(filters=200, kernel_size=4, activation='relu', padding='same')(x_text)\n",
    "        pool3 = layers.GlobalMaxPooling1D()(conv3)\n",
    "        conv_blocks.append(pool3)\n",
    "        \n",
    "        # Concatenar salidas de convoluciones\n",
    "        concat_text = layers.Concatenate()(conv_blocks)\n",
    "        \n",
    "        # Dropout (0.02 como en paper)\n",
    "        x_text = layers.Dropout(0.02)(concat_text)\n",
    "        \n",
    "        # Capa densa de 1000 unidades antes de fusión\n",
    "        text_features = layers.Dense(1000, activation='relu', name='text_features')(x_text)\n",
    "        \n",
    "        # ==================== RAMA IMAGEN ====================\n",
    "        image_input = layers.Input(shape=(224, 224, 3), name='image_input')\n",
    "        \n",
    "        # VGG16 desde CERO (NO pre-entrenado) - como en paper\n",
    "        base_model = VGG16(\n",
    "            weights=None,  # ⚠️ IMPORTANTE: Random initialization\n",
    "            include_top=False,\n",
    "            input_shape=(224, 224, 3)\n",
    "        )\n",
    "        \n",
    "        # Todas las capas entrenables (paper no menciona freezing)\n",
    "        for layer in base_model.layers:\n",
    "            layer.trainable = True\n",
    "        \n",
    "        x_image = base_model(image_input)\n",
    "        x_image = layers.GlobalAveragePooling2D()(x_image)\n",
    "        \n",
    "        # Capa fc2 de 1000 unidades (penultimate layer)\n",
    "        image_features = layers.Dense(1000, activation='relu', name='image_features')(x_image)\n",
    "        \n",
    "        # ==================== FUSIÓN MULTIMODAL ====================\n",
    "        # Concatenación (Early Fusion)\n",
    "        concatenated = layers.Concatenate(name='fusion')([text_features, image_features])\n",
    "        \n",
    "        # Capa oculta adicional después de fusión\n",
    "        hidden = layers.Dense(512, activation='relu')(concatenated)\n",
    "        hidden = layers.Dropout(0.02)(hidden)\n",
    "        \n",
    "        # Capa de salida final\n",
    "        output = layers.Dense(2, activation='softmax', name='output')(hidden)\n",
    "        \n",
    "        # Modelo final\n",
    "        model = models.Model(\n",
    "            inputs=[text_input, image_input], \n",
    "            outputs=output,\n",
    "            name='multimodal_paper_exact'\n",
    "        )\n",
    "    \n",
    "    return model\n",
    "\n",
    "# Crear modelo multimodal\n",
    "multimodal_model = create_multimodal_paper_exact(\n",
    "    vocab_size=vocab_size,\n",
    "    embedding_matrix=embedding_matrix,\n",
    "    max_seq_len=50\n",
    ")\n",
    "\n",
    "print(\"✅ Modelo multimodal creado (configuración exacta paper)\")\n",
    "multimodal_model.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e77332b7",
   "metadata": {},
   "source": [
    "### **Entrenamiento**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 133,
   "id": "31cb94ad",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iniciando entrenamiento multimodal según paper...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-11-26 06:05:14.711577: E external/local_xla/xla/stream_executor/gpu/gpu_cudamallocasync_allocator.cc:361] gpu_async_0 cuMemAllocAsync failed to allocate 4803649536 bytes: RESOURCE_EXHAUSTED: : CUDA_ERROR_OUT_OF_MEMORY: out of memory\n",
      " Reported by CUDA: Free memory/Total memory: 243007488/51041271808\n",
      "2025-11-26 06:05:14.711604: E external/local_xla/xla/stream_executor/gpu/gpu_cudamallocasync_allocator.cc:366] Stats: Limit:                       478937088\n",
      "InUse:                       171518048\n",
      "MaxInUse:                    269994416\n",
      "NumAllocs:                        1269\n",
      "MaxAllocSize:                 17199600\n",
      "Reserved:                            0\n",
      "PeakReserved:                        0\n",
      "LargestFreeBlock:                    0\n",
      "\n",
      "2025-11-26 06:05:14.711619: E external/local_xla/xla/stream_executor/gpu/gpu_cudamallocasync_allocator.cc:70] Histogram of current allocation: (allocation_size_in_bytes, nb_allocation_of_that_sizes), ...;\n",
      "2025-11-26 06:05:14.711622: E external/local_xla/xla/stream_executor/gpu/gpu_cudamallocasync_allocator.cc:73] 4, 73\n",
      "2025-11-26 06:05:14.711624: E external/local_xla/xla/stream_executor/gpu/gpu_cudamallocasync_allocator.cc:73] 8, 9\n",
      "2025-11-26 06:05:14.711626: E external/local_xla/xla/stream_executor/gpu/gpu_cudamallocasync_allocator.cc:73] 16, 4\n",
      "2025-11-26 06:05:14.711628: E external/local_xla/xla/stream_executor/gpu/gpu_cudamallocasync_allocator.cc:73] 256, 4\n",
      "2025-11-26 06:05:14.711629: E external/local_xla/xla/stream_executor/gpu/gpu_cudamallocasync_allocator.cc:73] 400, 2\n",
      "2025-11-26 06:05:14.711631: E external/local_xla/xla/stream_executor/gpu/gpu_cudamallocasync_allocator.cc:73] 512, 4\n",
      "2025-11-26 06:05:14.711633: E external/local_xla/xla/stream_executor/gpu/gpu_cudamallocasync_allocator.cc:73] 600, 2\n",
      "2025-11-26 06:05:14.711635: E external/local_xla/xla/stream_executor/gpu/gpu_cudamallocasync_allocator.cc:73] 800, 2\n",
      "2025-11-26 06:05:14.711636: E external/local_xla/xla/stream_executor/gpu/gpu_cudamallocasync_allocator.cc:73] 1024, 6\n",
      "2025-11-26 06:05:14.711638: E external/local_xla/xla/stream_executor/gpu/gpu_cudamallocasync_allocator.cc:73] 1028, 1\n",
      "2025-11-26 06:05:14.711640: E external/local_xla/xla/stream_executor/gpu/gpu_cudamallocasync_allocator.cc:73] 2048, 14\n",
      "2025-11-26 06:05:14.711641: E external/local_xla/xla/stream_executor/gpu/gpu_cudamallocasync_allocator.cc:73] 4000, 4\n",
      "2025-11-26 06:05:14.711643: E external/local_xla/xla/stream_executor/gpu/gpu_cudamallocasync_allocator.cc:73] 4096, 2\n",
      "2025-11-26 06:05:14.711645: E external/local_xla/xla/stream_executor/gpu/gpu_cudamallocasync_allocator.cc:73] 6912, 2\n",
      "2025-11-26 06:05:14.711647: E external/local_xla/xla/stream_executor/gpu/gpu_cudamallocasync_allocator.cc:73] 147456, 2\n",
      "2025-11-26 06:05:14.711648: E external/local_xla/xla/stream_executor/gpu/gpu_cudamallocasync_allocator.cc:73] 240000, 2\n",
      "2025-11-26 06:05:14.711650: E external/local_xla/xla/stream_executor/gpu/gpu_cudamallocasync_allocator.cc:73] 294912, 2\n",
      "2025-11-26 06:05:14.711652: E external/local_xla/xla/stream_executor/gpu/gpu_cudamallocasync_allocator.cc:73] 540000, 2\n",
      "2025-11-26 06:05:14.711653: E external/local_xla/xla/stream_executor/gpu/gpu_cudamallocasync_allocator.cc:73] 589824, 2\n",
      "2025-11-26 06:05:14.711655: E external/local_xla/xla/stream_executor/gpu/gpu_cudamallocasync_allocator.cc:73] 960000, 2\n",
      "2025-11-26 06:05:14.711657: E external/local_xla/xla/stream_executor/gpu/gpu_cudamallocasync_allocator.cc:73] 1179648, 2\n",
      "2025-11-26 06:05:14.711658: E external/local_xla/xla/stream_executor/gpu/gpu_cudamallocasync_allocator.cc:73] 1800000, 2\n",
      "2025-11-26 06:05:14.711660: E external/local_xla/xla/stream_executor/gpu/gpu_cudamallocasync_allocator.cc:73] 2048000, 2\n",
      "2025-11-26 06:05:14.711662: E external/local_xla/xla/stream_executor/gpu/gpu_cudamallocasync_allocator.cc:73] 2359296, 4\n",
      "2025-11-26 06:05:14.711664: E external/local_xla/xla/stream_executor/gpu/gpu_cudamallocasync_allocator.cc:73] 4096000, 2\n",
      "2025-11-26 06:05:14.711665: E external/local_xla/xla/stream_executor/gpu/gpu_cudamallocasync_allocator.cc:73] 4718592, 2\n",
      "2025-11-26 06:05:14.711667: E external/local_xla/xla/stream_executor/gpu/gpu_cudamallocasync_allocator.cc:73] 9437184, 10\n",
      "2025-11-26 06:05:14.711669: E external/local_xla/xla/stream_executor/gpu/gpu_cudamallocasync_allocator.cc:73] 17199600, 2\n",
      "2025-11-26 06:05:14.711672: E ext"
     ]
    },
    {
     "ename": "InternalError",
     "evalue": "Failed copying input tensor from /job:localhost/replica:0/task:0/device:CPU:0 to /job:localhost/replica:0/task:0/device:GPU:0 in order to run _EagerConst: Dst tensor is not initialized.",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mInternalError\u001b[39m                             Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[133]\u001b[39m\u001b[32m, line 48\u001b[39m\n\u001b[32m     45\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m history\n\u001b[32m     47\u001b[39m \u001b[38;5;66;03m# Entrenar modelo\u001b[39;00m\n\u001b[32m---> \u001b[39m\u001b[32m48\u001b[39m history = \u001b[43mcompile_and_train_multimodal_paper\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m     49\u001b[39m \u001b[43m    \u001b[49m\u001b[43mmultimodal_model\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m     50\u001b[39m \u001b[43m    \u001b[49m\u001b[43mX_text_train\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mX_image_train\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43my_train\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m     51\u001b[39m \u001b[43m    \u001b[49m\u001b[43mX_text_val\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mX_image_val\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43my_val\u001b[49m\n\u001b[32m     52\u001b[39m \u001b[43m)\u001b[49m\n",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[133]\u001b[39m\u001b[32m, line 35\u001b[39m, in \u001b[36mcompile_and_train_multimodal_paper\u001b[39m\u001b[34m(model, X_text_train, X_image_train, y_train, X_text_val, X_image_val, y_val)\u001b[39m\n\u001b[32m     33\u001b[39m \u001b[38;5;66;03m# Entrenar (50 épocas máximo como paper)\u001b[39;00m\n\u001b[32m     34\u001b[39m \u001b[38;5;28mprint\u001b[39m(\u001b[33m\"\u001b[39m\u001b[33mIniciando entrenamiento multimodal según paper...\u001b[39m\u001b[33m\"\u001b[39m)\n\u001b[32m---> \u001b[39m\u001b[32m35\u001b[39m history = \u001b[43mmodel\u001b[49m\u001b[43m.\u001b[49m\u001b[43mfit\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m     36\u001b[39m \u001b[43m    \u001b[49m\u001b[43mx\u001b[49m\u001b[43m=\u001b[49m\u001b[43m[\u001b[49m\u001b[43mX_text_train\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mX_image_train\u001b[49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m     37\u001b[39m \u001b[43m    \u001b[49m\u001b[43my\u001b[49m\u001b[43m=\u001b[49m\u001b[43my_train\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m     38\u001b[39m \u001b[43m    \u001b[49m\u001b[43mbatch_size\u001b[49m\u001b[43m=\u001b[49m\u001b[32;43m8\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m  \u001b[49m\u001b[38;5;66;43;03m# Minibatch size del paper: 32\u001b[39;49;00m\n\u001b[32m     39\u001b[39m \u001b[43m    \u001b[49m\u001b[43mepochs\u001b[49m\u001b[43m=\u001b[49m\u001b[32;43m50\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m      \u001b[49m\u001b[38;5;66;43;03m# Máximo según paper\u001b[39;49;00m\n\u001b[32m     40\u001b[39m \u001b[43m    \u001b[49m\u001b[43mvalidation_data\u001b[49m\u001b[43m=\u001b[49m\u001b[43m(\u001b[49m\u001b[43m[\u001b[49m\u001b[43mX_text_val\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mX_image_val\u001b[49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43my_val\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m     41\u001b[39m \u001b[43m    \u001b[49m\u001b[43mcallbacks\u001b[49m\u001b[43m=\u001b[49m\u001b[43mcallbacks\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m     42\u001b[39m \u001b[43m    \u001b[49m\u001b[43mverbose\u001b[49m\u001b[43m=\u001b[49m\u001b[32;43m1\u001b[39;49m\n\u001b[32m     43\u001b[39m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m     45\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m history\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/.conda/envs/crisis-env/lib/python3.13/site-packages/keras/src/utils/traceback_utils.py:122\u001b[39m, in \u001b[36mfilter_traceback.<locals>.error_handler\u001b[39m\u001b[34m(*args, **kwargs)\u001b[39m\n\u001b[32m    119\u001b[39m     filtered_tb = _process_traceback_frames(e.__traceback__)\n\u001b[32m    120\u001b[39m     \u001b[38;5;66;03m# To get the full stack trace, call:\u001b[39;00m\n\u001b[32m    121\u001b[39m     \u001b[38;5;66;03m# `keras.config.disable_traceback_filtering()`\u001b[39;00m\n\u001b[32m--> \u001b[39m\u001b[32m122\u001b[39m     \u001b[38;5;28;01mraise\u001b[39;00m e.with_traceback(filtered_tb) \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[32m    123\u001b[39m \u001b[38;5;28;01mfinally\u001b[39;00m:\n\u001b[32m    124\u001b[39m     \u001b[38;5;28;01mdel\u001b[39;00m filtered_tb\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/.conda/envs/crisis-env/lib/python3.13/site-packages/tensorflow/python/framework/constant_op.py:108\u001b[39m, in \u001b[36mconvert_to_eager_tensor\u001b[39m\u001b[34m(value, ctx, dtype)\u001b[39m\n\u001b[32m    106\u001b[39m     dtype = dtypes.as_dtype(dtype).as_datatype_enum\n\u001b[32m    107\u001b[39m ctx.ensure_initialized()\n\u001b[32m--> \u001b[39m\u001b[32m108\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mops\u001b[49m\u001b[43m.\u001b[49m\u001b[43mEagerTensor\u001b[49m\u001b[43m(\u001b[49m\u001b[43mvalue\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mctx\u001b[49m\u001b[43m.\u001b[49m\u001b[43mdevice_name\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdtype\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[31mInternalError\u001b[39m: Failed copying input tensor from /job:localhost/replica:0/task:0/device:CPU:0 to /job:localhost/replica:0/task:0/device:GPU:0 in order to run _EagerConst: Dst tensor is not initialized."
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "ernal/local_xla/xla/stream_executor/gpu/gpu_cudamallocasync_allocator.cc:106] CU_MEMPOOL_ATTR_RESERVED_MEM_CURRENT: 469762048\n",
      "2025-11-26 06:05:14.711676: E external/local_xla/xla/stream_executor/gpu/gpu_cudamallocasync_allocator.cc:108] CU_MEMPOOL_ATTR_USED_MEM_CURRENT: 171518048\n",
      "2025-11-26 06:05:14.711678: E external/local_xla/xla/stream_executor/gpu/gpu_cudamallocasync_allocator.cc:109] CU_MEMPOOL_ATTR_RESERVED_MEM_HIGH: 704643072\n",
      "2025-11-26 06:05:14.711680: E external/local_xla/xla/stream_executor/gpu/gpu_cudamallocasync_allocator.cc:110] CU_MEMPOOL_ATTR_USED_MEM_HIGH: 269994416\n"
     ]
    }
   ],
   "source": [
    "from tensorflow.keras.callbacks import EarlyStopping, ReduceLROnPlateau\n",
    "\n",
    "def compile_and_train_multimodal_paper(model, X_text_train, X_image_train, y_train, X_text_val, X_image_val, y_val):\n",
    "    \"\"\"\n",
    "    Compila y entrena el modelo EXACTO como paper\n",
    "    \"\"\"\n",
    "    # Compilar (Adam con learning rate por defecto como paper)\n",
    "    model.compile(\n",
    "        optimizer='adam',\n",
    "        loss='sparse_categorical_crossentropy',\n",
    "        metrics=['accuracy', 'precision', 'recall']\n",
    "    )\n",
    "    \n",
    "    # Callbacks como en paper\n",
    "    callbacks = [\n",
    "        # Early stopping con patience=10 (página 5 del paper)\n",
    "        EarlyStopping(\n",
    "            monitor='val_accuracy',\n",
    "            patience=10,\n",
    "            restore_best_weights=True,\n",
    "            verbose=1\n",
    "        ),\n",
    "        # Paper menciona reducir learning rate pero no especifica parámetros\n",
    "        ReduceLROnPlateau(\n",
    "            monitor='val_loss',\n",
    "            factor=0.5,\n",
    "            patience=5,\n",
    "            min_lr=1e-7,\n",
    "            verbose=1\n",
    "        )\n",
    "    ]\n",
    "    \n",
    "    # Entrenar (50 épocas máximo como paper)\n",
    "    print(\"Iniciando entrenamiento multimodal según paper...\")\n",
    "    history = model.fit(\n",
    "        x=[X_text_train, X_image_train],\n",
    "        y=y_train,\n",
    "        batch_size=8,  # Minibatch size del paper: 32\n",
    "        epochs=50,      # Máximo según paper\n",
    "        validation_data=([X_text_val, X_image_val], y_val),\n",
    "        callbacks=callbacks,\n",
    "        verbose=1\n",
    "    )\n",
    "    \n",
    "    return history\n",
    "\n",
    "# Entrenar modelo\n",
    "history = compile_and_train_multimodal_paper(\n",
    "    multimodal_model,\n",
    "    X_text_train, X_image_train, y_train,\n",
    "    X_text_val, X_image_val, y_val\n",
    ")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python (Crisis GPU)",
   "language": "python",
   "name": "crisis-env"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
