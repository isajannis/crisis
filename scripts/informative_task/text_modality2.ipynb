{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "309e30cd",
   "metadata": {},
   "source": [
    "## **CNN: Text Modality**\n",
    "\n",
    "Se usa una arquitectura que consiste en redes neuronales convolucionales. En el paper se menciona el uso de un modelo word2vec pre-entrenado discutido en otro paper, como no se tiene acceso a este modelo se prefirió optar por otras alternativas similares pero manteniendo el uso de word2vec con 300 dimensiones."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "3a3b9b37",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: tensorflow in c:\\users\\isaja\\documents\\dcc\\e\\crisis\\env\\lib\\site-packages (2.20.0)\n",
      "Requirement already satisfied: absl-py>=1.0.0 in c:\\users\\isaja\\documents\\dcc\\e\\crisis\\env\\lib\\site-packages (from tensorflow) (2.3.1)\n",
      "Requirement already satisfied: astunparse>=1.6.0 in c:\\users\\isaja\\documents\\dcc\\e\\crisis\\env\\lib\\site-packages (from tensorflow) (1.6.3)\n",
      "Requirement already satisfied: flatbuffers>=24.3.25 in c:\\users\\isaja\\documents\\dcc\\e\\crisis\\env\\lib\\site-packages (from tensorflow) (25.9.23)\n",
      "Requirement already satisfied: gast!=0.5.0,!=0.5.1,!=0.5.2,>=0.2.1 in c:\\users\\isaja\\documents\\dcc\\e\\crisis\\env\\lib\\site-packages (from tensorflow) (0.6.0)\n",
      "Requirement already satisfied: google_pasta>=0.1.1 in c:\\users\\isaja\\documents\\dcc\\e\\crisis\\env\\lib\\site-packages (from tensorflow) (0.2.0)\n",
      "Requirement already satisfied: libclang>=13.0.0 in c:\\users\\isaja\\documents\\dcc\\e\\crisis\\env\\lib\\site-packages (from tensorflow) (18.1.1)\n",
      "Requirement already satisfied: opt_einsum>=2.3.2 in c:\\users\\isaja\\documents\\dcc\\e\\crisis\\env\\lib\\site-packages (from tensorflow) (3.4.0)\n",
      "Requirement already satisfied: packaging in c:\\users\\isaja\\documents\\dcc\\e\\crisis\\env\\lib\\site-packages (from tensorflow) (25.0)\n",
      "Requirement already satisfied: protobuf>=5.28.0 in c:\\users\\isaja\\documents\\dcc\\e\\crisis\\env\\lib\\site-packages (from tensorflow) (6.33.0)\n",
      "Requirement already satisfied: requests<3,>=2.21.0 in c:\\users\\isaja\\documents\\dcc\\e\\crisis\\env\\lib\\site-packages (from tensorflow) (2.32.5)\n",
      "Requirement already satisfied: setuptools in c:\\users\\isaja\\documents\\dcc\\e\\crisis\\env\\lib\\site-packages (from tensorflow) (80.9.0)\n",
      "Requirement already satisfied: six>=1.12.0 in c:\\users\\isaja\\documents\\dcc\\e\\crisis\\env\\lib\\site-packages (from tensorflow) (1.17.0)\n",
      "Requirement already satisfied: termcolor>=1.1.0 in c:\\users\\isaja\\documents\\dcc\\e\\crisis\\env\\lib\\site-packages (from tensorflow) (3.1.0)\n",
      "Requirement already satisfied: typing_extensions>=3.6.6 in c:\\users\\isaja\\documents\\dcc\\e\\crisis\\env\\lib\\site-packages (from tensorflow) (4.15.0)\n",
      "Requirement already satisfied: wrapt>=1.11.0 in c:\\users\\isaja\\documents\\dcc\\e\\crisis\\env\\lib\\site-packages (from tensorflow) (2.0.0)\n",
      "Requirement already satisfied: grpcio<2.0,>=1.24.3 in c:\\users\\isaja\\documents\\dcc\\e\\crisis\\env\\lib\\site-packages (from tensorflow) (1.75.1)\n",
      "Requirement already satisfied: tensorboard~=2.20.0 in c:\\users\\isaja\\documents\\dcc\\e\\crisis\\env\\lib\\site-packages (from tensorflow) (2.20.0)\n",
      "Requirement already satisfied: keras>=3.10.0 in c:\\users\\isaja\\documents\\dcc\\e\\crisis\\env\\lib\\site-packages (from tensorflow) (3.11.3)\n",
      "Requirement already satisfied: numpy>=1.26.0 in c:\\users\\isaja\\documents\\dcc\\e\\crisis\\env\\lib\\site-packages (from tensorflow) (2.2.6)\n",
      "Requirement already satisfied: h5py>=3.11.0 in c:\\users\\isaja\\documents\\dcc\\e\\crisis\\env\\lib\\site-packages (from tensorflow) (3.15.1)\n",
      "Requirement already satisfied: ml_dtypes<1.0.0,>=0.5.1 in c:\\users\\isaja\\documents\\dcc\\e\\crisis\\env\\lib\\site-packages (from tensorflow) (0.5.3)\n",
      "Requirement already satisfied: charset_normalizer<4,>=2 in c:\\users\\isaja\\documents\\dcc\\e\\crisis\\env\\lib\\site-packages (from requests<3,>=2.21.0->tensorflow) (3.4.4)\n",
      "Requirement already satisfied: idna<4,>=2.5 in c:\\users\\isaja\\documents\\dcc\\e\\crisis\\env\\lib\\site-packages (from requests<3,>=2.21.0->tensorflow) (3.11)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in c:\\users\\isaja\\documents\\dcc\\e\\crisis\\env\\lib\\site-packages (from requests<3,>=2.21.0->tensorflow) (2.5.0)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in c:\\users\\isaja\\documents\\dcc\\e\\crisis\\env\\lib\\site-packages (from requests<3,>=2.21.0->tensorflow) (2025.10.5)\n",
      "Requirement already satisfied: markdown>=2.6.8 in c:\\users\\isaja\\documents\\dcc\\e\\crisis\\env\\lib\\site-packages (from tensorboard~=2.20.0->tensorflow) (3.9)\n",
      "Requirement already satisfied: pillow in c:\\users\\isaja\\documents\\dcc\\e\\crisis\\env\\lib\\site-packages (from tensorboard~=2.20.0->tensorflow) (12.0.0)\n",
      "Requirement already satisfied: tensorboard-data-server<0.8.0,>=0.7.0 in c:\\users\\isaja\\documents\\dcc\\e\\crisis\\env\\lib\\site-packages (from tensorboard~=2.20.0->tensorflow) (0.7.2)\n",
      "Requirement already satisfied: werkzeug>=1.0.1 in c:\\users\\isaja\\documents\\dcc\\e\\crisis\\env\\lib\\site-packages (from tensorboard~=2.20.0->tensorflow) (3.1.3)\n",
      "Requirement already satisfied: wheel<1.0,>=0.23.0 in c:\\users\\isaja\\documents\\dcc\\e\\crisis\\env\\lib\\site-packages (from astunparse>=1.6.0->tensorflow) (0.45.1)\n",
      "Requirement already satisfied: rich in c:\\users\\isaja\\documents\\dcc\\e\\crisis\\env\\lib\\site-packages (from keras>=3.10.0->tensorflow) (14.2.0)\n",
      "Requirement already satisfied: namex in c:\\users\\isaja\\documents\\dcc\\e\\crisis\\env\\lib\\site-packages (from keras>=3.10.0->tensorflow) (0.1.0)\n",
      "Requirement already satisfied: optree in c:\\users\\isaja\\documents\\dcc\\e\\crisis\\env\\lib\\site-packages (from keras>=3.10.0->tensorflow) (0.17.0)\n",
      "Requirement already satisfied: MarkupSafe>=2.1.1 in c:\\users\\isaja\\documents\\dcc\\e\\crisis\\env\\lib\\site-packages (from werkzeug>=1.0.1->tensorboard~=2.20.0->tensorflow) (3.0.3)\n",
      "Requirement already satisfied: markdown-it-py>=2.2.0 in c:\\users\\isaja\\documents\\dcc\\e\\crisis\\env\\lib\\site-packages (from rich->keras>=3.10.0->tensorflow) (4.0.0)\n",
      "Requirement already satisfied: pygments<3.0.0,>=2.13.0 in c:\\users\\isaja\\documents\\dcc\\e\\crisis\\env\\lib\\site-packages (from rich->keras>=3.10.0->tensorflow) (2.19.2)\n",
      "Requirement already satisfied: mdurl~=0.1 in c:\\users\\isaja\\documents\\dcc\\e\\crisis\\env\\lib\\site-packages (from markdown-it-py>=2.2.0->rich->keras>=3.10.0->tensorflow) (0.1.2)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "[notice] A new release of pip is available: 25.2 -> 25.3\n",
      "[notice] To update, run: python.exe -m pip install --upgrade pip\n"
     ]
    }
   ],
   "source": [
    "!pip install tensorflow\n",
    "import tensorflow.keras"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "34681525",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: nltk in c:\\users\\isaja\\documents\\dcc\\e\\crisis\\env\\lib\\site-packages (3.9.2)\n",
      "Requirement already satisfied: click in c:\\users\\isaja\\documents\\dcc\\e\\crisis\\env\\lib\\site-packages (from nltk) (8.3.0)\n",
      "Requirement already satisfied: joblib in c:\\users\\isaja\\documents\\dcc\\e\\crisis\\env\\lib\\site-packages (from nltk) (1.5.2)\n",
      "Requirement already satisfied: regex>=2021.8.3 in c:\\users\\isaja\\documents\\dcc\\e\\crisis\\env\\lib\\site-packages (from nltk) (2025.9.18)\n",
      "Requirement already satisfied: tqdm in c:\\users\\isaja\\documents\\dcc\\e\\crisis\\env\\lib\\site-packages (from nltk) (4.67.1)\n",
      "Requirement already satisfied: colorama in c:\\users\\isaja\\documents\\dcc\\e\\crisis\\env\\lib\\site-packages (from click->nltk) (0.4.6)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "[notice] A new release of pip is available: 25.2 -> 25.3\n",
      "[notice] To update, run: python.exe -m pip install --upgrade pip\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: tensorflow in c:\\users\\isaja\\documents\\dcc\\e\\crisis\\env\\lib\\site-packages (2.20.0)\n",
      "Requirement already satisfied: absl-py>=1.0.0 in c:\\users\\isaja\\documents\\dcc\\e\\crisis\\env\\lib\\site-packages (from tensorflow) (2.3.1)\n",
      "Requirement already satisfied: astunparse>=1.6.0 in c:\\users\\isaja\\documents\\dcc\\e\\crisis\\env\\lib\\site-packages (from tensorflow) (1.6.3)\n",
      "Requirement already satisfied: flatbuffers>=24.3.25 in c:\\users\\isaja\\documents\\dcc\\e\\crisis\\env\\lib\\site-packages (from tensorflow) (25.9.23)\n",
      "Requirement already satisfied: gast!=0.5.0,!=0.5.1,!=0.5.2,>=0.2.1 in c:\\users\\isaja\\documents\\dcc\\e\\crisis\\env\\lib\\site-packages (from tensorflow) (0.6.0)\n",
      "Requirement already satisfied: google_pasta>=0.1.1 in c:\\users\\isaja\\documents\\dcc\\e\\crisis\\env\\lib\\site-packages (from tensorflow) (0.2.0)\n",
      "Requirement already satisfied: libclang>=13.0.0 in c:\\users\\isaja\\documents\\dcc\\e\\crisis\\env\\lib\\site-packages (from tensorflow) (18.1.1)\n",
      "Requirement already satisfied: opt_einsum>=2.3.2 in c:\\users\\isaja\\documents\\dcc\\e\\crisis\\env\\lib\\site-packages (from tensorflow) (3.4.0)\n",
      "Requirement already satisfied: packaging in c:\\users\\isaja\\documents\\dcc\\e\\crisis\\env\\lib\\site-packages (from tensorflow) (25.0)\n",
      "Requirement already satisfied: protobuf>=5.28.0 in c:\\users\\isaja\\documents\\dcc\\e\\crisis\\env\\lib\\site-packages (from tensorflow) (6.33.0)\n",
      "Requirement already satisfied: requests<3,>=2.21.0 in c:\\users\\isaja\\documents\\dcc\\e\\crisis\\env\\lib\\site-packages (from tensorflow) (2.32.5)\n",
      "Requirement already satisfied: setuptools in c:\\users\\isaja\\documents\\dcc\\e\\crisis\\env\\lib\\site-packages (from tensorflow) (80.9.0)\n",
      "Requirement already satisfied: six>=1.12.0 in c:\\users\\isaja\\documents\\dcc\\e\\crisis\\env\\lib\\site-packages (from tensorflow) (1.17.0)\n",
      "Requirement already satisfied: termcolor>=1.1.0 in c:\\users\\isaja\\documents\\dcc\\e\\crisis\\env\\lib\\site-packages (from tensorflow) (3.1.0)\n",
      "Requirement already satisfied: typing_extensions>=3.6.6 in c:\\users\\isaja\\documents\\dcc\\e\\crisis\\env\\lib\\site-packages (from tensorflow) (4.15.0)\n",
      "Requirement already satisfied: wrapt>=1.11.0 in c:\\users\\isaja\\documents\\dcc\\e\\crisis\\env\\lib\\site-packages (from tensorflow) (2.0.0)\n",
      "Requirement already satisfied: grpcio<2.0,>=1.24.3 in c:\\users\\isaja\\documents\\dcc\\e\\crisis\\env\\lib\\site-packages (from tensorflow) (1.75.1)\n",
      "Requirement already satisfied: tensorboard~=2.20.0 in c:\\users\\isaja\\documents\\dcc\\e\\crisis\\env\\lib\\site-packages (from tensorflow) (2.20.0)\n",
      "Requirement already satisfied: keras>=3.10.0 in c:\\users\\isaja\\documents\\dcc\\e\\crisis\\env\\lib\\site-packages (from tensorflow) (3.11.3)\n",
      "Requirement already satisfied: numpy>=1.26.0 in c:\\users\\isaja\\documents\\dcc\\e\\crisis\\env\\lib\\site-packages (from tensorflow) (2.2.6)\n",
      "Requirement already satisfied: h5py>=3.11.0 in c:\\users\\isaja\\documents\\dcc\\e\\crisis\\env\\lib\\site-packages (from tensorflow) (3.15.1)\n",
      "Requirement already satisfied: ml_dtypes<1.0.0,>=0.5.1 in c:\\users\\isaja\\documents\\dcc\\e\\crisis\\env\\lib\\site-packages (from tensorflow) (0.5.3)\n",
      "Requirement already satisfied: charset_normalizer<4,>=2 in c:\\users\\isaja\\documents\\dcc\\e\\crisis\\env\\lib\\site-packages (from requests<3,>=2.21.0->tensorflow) (3.4.4)\n",
      "Requirement already satisfied: idna<4,>=2.5 in c:\\users\\isaja\\documents\\dcc\\e\\crisis\\env\\lib\\site-packages (from requests<3,>=2.21.0->tensorflow) (3.11)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in c:\\users\\isaja\\documents\\dcc\\e\\crisis\\env\\lib\\site-packages (from requests<3,>=2.21.0->tensorflow) (2.5.0)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in c:\\users\\isaja\\documents\\dcc\\e\\crisis\\env\\lib\\site-packages (from requests<3,>=2.21.0->tensorflow) (2025.10.5)\n",
      "Requirement already satisfied: markdown>=2.6.8 in c:\\users\\isaja\\documents\\dcc\\e\\crisis\\env\\lib\\site-packages (from tensorboard~=2.20.0->tensorflow) (3.9)\n",
      "Requirement already satisfied: pillow in c:\\users\\isaja\\documents\\dcc\\e\\crisis\\env\\lib\\site-packages (from tensorboard~=2.20.0->tensorflow) (12.0.0)\n",
      "Requirement already satisfied: tensorboard-data-server<0.8.0,>=0.7.0 in c:\\users\\isaja\\documents\\dcc\\e\\crisis\\env\\lib\\site-packages (from tensorboard~=2.20.0->tensorflow) (0.7.2)\n",
      "Requirement already satisfied: werkzeug>=1.0.1 in c:\\users\\isaja\\documents\\dcc\\e\\crisis\\env\\lib\\site-packages (from tensorboard~=2.20.0->tensorflow) (3.1.3)\n",
      "Requirement already satisfied: wheel<1.0,>=0.23.0 in c:\\users\\isaja\\documents\\dcc\\e\\crisis\\env\\lib\\site-packages (from astunparse>=1.6.0->tensorflow) (0.45.1)\n",
      "Requirement already satisfied: rich in c:\\users\\isaja\\documents\\dcc\\e\\crisis\\env\\lib\\site-packages (from keras>=3.10.0->tensorflow) (14.2.0)\n",
      "Requirement already satisfied: namex in c:\\users\\isaja\\documents\\dcc\\e\\crisis\\env\\lib\\site-packages (from keras>=3.10.0->tensorflow) (0.1.0)\n",
      "Requirement already satisfied: optree in c:\\users\\isaja\\documents\\dcc\\e\\crisis\\env\\lib\\site-packages (from keras>=3.10.0->tensorflow) (0.17.0)\n",
      "Requirement already satisfied: MarkupSafe>=2.1.1 in c:\\users\\isaja\\documents\\dcc\\e\\crisis\\env\\lib\\site-packages (from werkzeug>=1.0.1->tensorboard~=2.20.0->tensorflow) (3.0.3)\n",
      "Requirement already satisfied: markdown-it-py>=2.2.0 in c:\\users\\isaja\\documents\\dcc\\e\\crisis\\env\\lib\\site-packages (from rich->keras>=3.10.0->tensorflow) (4.0.0)\n",
      "Requirement already satisfied: pygments<3.0.0,>=2.13.0 in c:\\users\\isaja\\documents\\dcc\\e\\crisis\\env\\lib\\site-packages (from rich->keras>=3.10.0->tensorflow) (2.19.2)\n",
      "Requirement already satisfied: mdurl~=0.1 in c:\\users\\isaja\\documents\\dcc\\e\\crisis\\env\\lib\\site-packages (from markdown-it-py>=2.2.0->rich->keras>=3.10.0->tensorflow) (0.1.2)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "[notice] A new release of pip is available: 25.2 -> 25.3\n",
      "[notice] To update, run: python.exe -m pip install --upgrade pip\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: torch in c:\\users\\isaja\\documents\\dcc\\e\\crisis\\env\\lib\\site-packages (2.9.0)\n",
      "Requirement already satisfied: filelock in c:\\users\\isaja\\documents\\dcc\\e\\crisis\\env\\lib\\site-packages (from torch) (3.20.0)\n",
      "Requirement already satisfied: typing-extensions>=4.10.0 in c:\\users\\isaja\\documents\\dcc\\e\\crisis\\env\\lib\\site-packages (from torch) (4.15.0)\n",
      "Requirement already satisfied: sympy>=1.13.3 in c:\\users\\isaja\\documents\\dcc\\e\\crisis\\env\\lib\\site-packages (from torch) (1.14.0)\n",
      "Requirement already satisfied: networkx>=2.5.1 in c:\\users\\isaja\\documents\\dcc\\e\\crisis\\env\\lib\\site-packages (from torch) (3.5)\n",
      "Requirement already satisfied: jinja2 in c:\\users\\isaja\\documents\\dcc\\e\\crisis\\env\\lib\\site-packages (from torch) (3.1.6)\n",
      "Requirement already satisfied: fsspec>=0.8.5 in c:\\users\\isaja\\documents\\dcc\\e\\crisis\\env\\lib\\site-packages (from torch) (2025.9.0)\n",
      "Requirement already satisfied: setuptools in c:\\users\\isaja\\documents\\dcc\\e\\crisis\\env\\lib\\site-packages (from torch) (80.9.0)\n",
      "Requirement already satisfied: mpmath<1.4,>=1.1.0 in c:\\users\\isaja\\documents\\dcc\\e\\crisis\\env\\lib\\site-packages (from sympy>=1.13.3->torch) (1.3.0)\n",
      "Requirement already satisfied: MarkupSafe>=2.0 in c:\\users\\isaja\\documents\\dcc\\e\\crisis\\env\\lib\\site-packages (from jinja2->torch) (3.0.3)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "[notice] A new release of pip is available: 25.2 -> 25.3\n",
      "[notice] To update, run: python.exe -m pip install --upgrade pip\n"
     ]
    }
   ],
   "source": [
    "!pip install nltk\n",
    "!pip install tensorflow\n",
    "!pip install torch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "7854864d",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import re"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fc87690b",
   "metadata": {},
   "source": [
    "### **Preprocesamiento**\n",
    "\"we preprocess them by removing stop words, non-ASCII charachters, numbers, URLs and hashtag signs. we also replace all punctuation marks with white spaces\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "3ff5ae47",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>tweet_id</th>\n",
       "      <th>text_info</th>\n",
       "      <th>text_info_conf</th>\n",
       "      <th>tweet_text</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>917791044158185473</td>\n",
       "      <td>informative</td>\n",
       "      <td>1.0000</td>\n",
       "      <td>RT @Gizmodo: Wildfires raging through Northern...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>917791130590183424</td>\n",
       "      <td>informative</td>\n",
       "      <td>1.0000</td>\n",
       "      <td>PHOTOS: Deadly wildfires rage in California ht...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>917791291823591425</td>\n",
       "      <td>informative</td>\n",
       "      <td>0.6813</td>\n",
       "      <td>RT @Cal_OES: PLS SHARE: Weâ€™re capturing wild...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>917792092100988929</td>\n",
       "      <td>informative</td>\n",
       "      <td>0.6727</td>\n",
       "      <td>RT @TIME: California's raging wildfires as you...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>917792147700465664</td>\n",
       "      <td>informative</td>\n",
       "      <td>0.7143</td>\n",
       "      <td>Wildfires Threaten Californiaâ€™s First Legal ...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "             tweet_id    text_info  text_info_conf  \\\n",
       "0  917791044158185473  informative          1.0000   \n",
       "1  917791130590183424  informative          1.0000   \n",
       "2  917791291823591425  informative          0.6813   \n",
       "3  917792092100988929  informative          0.6727   \n",
       "4  917792147700465664  informative          0.7143   \n",
       "\n",
       "                                          tweet_text  \n",
       "0  RT @Gizmodo: Wildfires raging through Northern...  \n",
       "1  PHOTOS: Deadly wildfires rage in California ht...  \n",
       "2  RT @Cal_OES: PLS SHARE: Weâ€™re capturing wild...  \n",
       "3  RT @TIME: California's raging wildfires as you...  \n",
       "4  Wildfires Threaten Californiaâ€™s First Legal ...  "
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_text = pd.read_csv('../data/crisis_texts_dataset.csv')\n",
    "df_text.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "aae30523",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'don', 'further', 'until', 'in', 'most', 'o', 'no', 'nor', \"they'll\", 'on', \"needn't\", 'am', 'or', \"that'll\", \"she'll\", 'into', 'a', 've', 'where', 'the', 'can', 'if', 'weren', \"we've\", 'of', 'while', 'didn', 'for', 'ma', 'does', 'him', 'too', 'same', 'shouldn', 'their', 'hers', 'from', 'hadn', \"they'd\", 'is', 'again', 'yours', 'between', 'aren', 'about', 'has', 're', 'which', 'whom', 'through', 'haven', \"should've\", 'themselves', 'by', 'couldn', 'out', 'other', 'do', 'there', 'itself', 'just', 'once', 'ourselves', 'y', 'shan', 'being', 'we', \"you'll\", 'mightn', 'not', 'here', 'isn', \"isn't\", 'll', \"we're\", 'did', \"you've\", 'it', 'they', \"i'll\", 'had', 'its', 'very', 'mustn', 'up', 'then', 'as', 'any', 'against', 'm', 'how', \"they're\", 'your', \"i've\", 'ours', \"it'd\", 'what', 'wouldn', 'his', \"wouldn't\", 'are', 'each', \"haven't\", 'these', 'this', \"you're\", 'theirs', \"they've\", \"couldn't\", \"she's\", 'now', 'all', 'doesn', 'yourselves', 'ain', 'an', \"hadn't\", 'have', \"it's\", 'below', 'be', 'at', 'hasn', \"don't\", 'because', \"she'd\", 'that', 'needn', 'down', \"didn't\", 't', 'i', \"we'll\", 'during', \"i'm\", 'such', 'with', 'before', 'she', 'only', \"wasn't\", \"shan't\", 'those', 'd', 'himself', 'our', 'won', 'was', 'some', 'been', 'should', \"shouldn't\", \"he's\", \"it'll\", 'were', 'than', 'but', 'off', 'after', \"we'd\", 'own', 'yourself', 'to', 'rt', \"hasn't\", 's', 'having', \"aren't\", 'myself', \"mightn't\", \"won't\", \"mustn't\", 'he', 'herself', 'will', \"i'd\", \"he'll\", 'under', 'above', 'over', 'her', 'both', \"doesn't\", 'RT', 'you', 'me', 'doing', \"weren't\", \"you'd\", \"he'd\", 'more', 'so', 'my', 'few', 'who', 'them', 'wasn', 'why', 'when', 'and'}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     C:\\Users\\isaja\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "# Para el manejo de stopwords\n",
    "import nltk\n",
    "from nltk.corpus import stopwords\n",
    "nltk.download('stopwords')\n",
    "stop_words = set(stopwords.words('english'))\n",
    "stop_words.update(['rt', 'RT'])\n",
    "print(stop_words)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "28224fcd",
   "metadata": {},
   "outputs": [],
   "source": [
    "def clean_text(text):\n",
    "    # Eliminar URLs\n",
    "    text = re.sub(r'http\\S+|www\\S+|https\\S+', '', text, flags=re.MULTILINE)\n",
    "    # Eliminar caracteres no ASCII\n",
    "    text = text.encode('ascii', 'ignore').decode('ascii')\n",
    "    # Eliminar números\n",
    "    text = re.sub(r'\\d+', '', text)\n",
    "    # Reemplazar signos de puntuación con espacios\n",
    "    text = re.sub(r'[^\\w\\s]', ' ', text)\n",
    "    # Convertir a minúsculas y eliminar stopwords\n",
    "    text = ' '.join([word.lower() for word in text.split() if word.lower() not in stop_words])\n",
    "    return text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "165f6d2c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'alooooooo happy'"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "clean_text(\"RT alooooooo #happy http://example.com 123!!!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "2314c7d0",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>tweet_id</th>\n",
       "      <th>text_info</th>\n",
       "      <th>text_info_conf</th>\n",
       "      <th>tweet_text</th>\n",
       "      <th>clean_text</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>917791044158185473</td>\n",
       "      <td>informative</td>\n",
       "      <td>1.0000</td>\n",
       "      <td>RT @Gizmodo: Wildfires raging through Northern...</td>\n",
       "      <td>gizmodo wildfires raging northern california t...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>917791130590183424</td>\n",
       "      <td>informative</td>\n",
       "      <td>1.0000</td>\n",
       "      <td>PHOTOS: Deadly wildfires rage in California ht...</td>\n",
       "      <td>photos deadly wildfires rage california</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>917791291823591425</td>\n",
       "      <td>informative</td>\n",
       "      <td>0.6813</td>\n",
       "      <td>RT @Cal_OES: PLS SHARE: Weâ€™re capturing wild...</td>\n",
       "      <td>cal_oes pls share capturing wildfire response ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>917792092100988929</td>\n",
       "      <td>informative</td>\n",
       "      <td>0.6727</td>\n",
       "      <td>RT @TIME: California's raging wildfires as you...</td>\n",
       "      <td>time california raging wildfires never seen</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>917792147700465664</td>\n",
       "      <td>informative</td>\n",
       "      <td>0.7143</td>\n",
       "      <td>Wildfires Threaten Californiaâ€™s First Legal ...</td>\n",
       "      <td>wildfires threaten californias first legal can...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>95</th>\n",
       "      <td>917833987824979968</td>\n",
       "      <td>informative</td>\n",
       "      <td>0.6783</td>\n",
       "      <td>Satellite image of California last night. Thos...</td>\n",
       "      <td>satellite image california last night arent li...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>96</th>\n",
       "      <td>917834588814155776</td>\n",
       "      <td>informative</td>\n",
       "      <td>0.6854</td>\n",
       "      <td>RT @AuroraWorldView: Deadly #California #wildf...</td>\n",
       "      <td>auroraworldview deadly california wildfires fo...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>97</th>\n",
       "      <td>917834628290920448</td>\n",
       "      <td>informative</td>\n",
       "      <td>0.6667</td>\n",
       "      <td>Fire swept through Northern California early M...</td>\n",
       "      <td>fire swept northern california early monday mo...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>98</th>\n",
       "      <td>917835067069788162</td>\n",
       "      <td>informative</td>\n",
       "      <td>1.0000</td>\n",
       "      <td>Couple, ages 100 and 98, die in CaliforniaÂ wi...</td>\n",
       "      <td>couple ages die california wildfire</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>99</th>\n",
       "      <td>917835123890049025</td>\n",
       "      <td>informative</td>\n",
       "      <td>0.6436</td>\n",
       "      <td>Napa Fire Map: Napa Valley Timeline, Death Tol...</td>\n",
       "      <td>napa fire map napa valley timeline death toll ...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>100 rows × 5 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "              tweet_id    text_info  text_info_conf  \\\n",
       "0   917791044158185473  informative          1.0000   \n",
       "1   917791130590183424  informative          1.0000   \n",
       "2   917791291823591425  informative          0.6813   \n",
       "3   917792092100988929  informative          0.6727   \n",
       "4   917792147700465664  informative          0.7143   \n",
       "..                 ...          ...             ...   \n",
       "95  917833987824979968  informative          0.6783   \n",
       "96  917834588814155776  informative          0.6854   \n",
       "97  917834628290920448  informative          0.6667   \n",
       "98  917835067069788162  informative          1.0000   \n",
       "99  917835123890049025  informative          0.6436   \n",
       "\n",
       "                                           tweet_text  \\\n",
       "0   RT @Gizmodo: Wildfires raging through Northern...   \n",
       "1   PHOTOS: Deadly wildfires rage in California ht...   \n",
       "2   RT @Cal_OES: PLS SHARE: Weâ€™re capturing wild...   \n",
       "3   RT @TIME: California's raging wildfires as you...   \n",
       "4   Wildfires Threaten Californiaâ€™s First Legal ...   \n",
       "..                                                ...   \n",
       "95  Satellite image of California last night. Thos...   \n",
       "96  RT @AuroraWorldView: Deadly #California #wildf...   \n",
       "97  Fire swept through Northern California early M...   \n",
       "98  Couple, ages 100 and 98, die in CaliforniaÂ wi...   \n",
       "99  Napa Fire Map: Napa Valley Timeline, Death Tol...   \n",
       "\n",
       "                                           clean_text  \n",
       "0   gizmodo wildfires raging northern california t...  \n",
       "1             photos deadly wildfires rage california  \n",
       "2   cal_oes pls share capturing wildfire response ...  \n",
       "3         time california raging wildfires never seen  \n",
       "4   wildfires threaten californias first legal can...  \n",
       "..                                                ...  \n",
       "95  satellite image california last night arent li...  \n",
       "96  auroraworldview deadly california wildfires fo...  \n",
       "97  fire swept northern california early monday mo...  \n",
       "98                couple ages die california wildfire  \n",
       "99  napa fire map napa valley timeline death toll ...  \n",
       "\n",
       "[100 rows x 5 columns]"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_text['clean_text'] = df_text['tweet_text'].apply(clean_text)\n",
    "df_text.head(100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "8c17024b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['officials disagree puerto rico outage restoration timeline',\n",
       " 'california wildfires video shows extreme damage caused flames abc arizona',\n",
       " 'us photographer shot mexico earthquake hurt',\n",
       " 'mama cousins house pr praying affected hurricane maria fc',\n",
       " 'days ago irma jose katia jose lee maria',\n",
       " 'residents feel like nd class citizens hurricane maria citizenliveat waihigamwaura',\n",
       " 'clean efforts way wake irma',\n",
       " 'money irma cnnpolitics republicans done obamacare repeal cnn politics',\n",
       " 'assesment going mora effected sonoya gandamara shekerkhil community baskhali upazilla chittagong',\n",
       " 'puerto ricos main airport heavy hearts long waits']"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Muestra aleatoria de textos limpios\n",
    "import random\n",
    "random.sample(list(df_text['clean_text']), 10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "55c0d821",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(('informative', 'gizmodo wildfires raging northern california terrifying'),\n",
       " ('informative', 'photos deadly wildfires rage california'),\n",
       " ('informative',\n",
       "  'cal_oes pls share capturing wildfire response recovery info'),\n",
       " ('informative', 'time california raging wildfires never seen'),\n",
       " ('informative',\n",
       "  'wildfires threaten californias first legal cannabis harvest'))"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#dataset = tuple((row['informative'], row['clean_text']) for _, row in df_text.iterrows())\n",
    "dataset = tuple(\n",
    "    (row['text_info'], row['clean_text'])\n",
    "    for _, row in df_text.iterrows() \n",
    ")\n",
    "dataset[:5]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "f0804b8c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tamaño del set de entrenamiento: 12043\n",
      "Tamaño del set de validación: 2408\n",
      "Tamaño del set de prueba: 1607\n",
      "\n",
      "Ejemplos del set de entrenamiento:\n",
      "('informative', 'nh donation drive puerto rico exceeds expectations')\n",
      "('informative', 'nasagoddard nasa measures hurricane maria torrential rainfall sees eye open')\n",
      "('informative', 'airmen kentucky ang deployed caribbean hurricanemaria relief operations')\n",
      "\n",
      "Ejemplos del set de validación:\n",
      "('informative', 'puertorico needs unprecedented aid hurricanemaria recovery says oversight board')\n",
      "('not_informative', 'irma nfl stafford ebron lions take lead onepride detvsnyg via twitt')\n",
      "('informative', 'tropical storm irma damaged south carolina coastal communities chsnews scnews chswx')\n",
      "\n",
      "Ejemplos del set de prueba:\n",
      "('informative', 'disaster california fire get help get help fire')\n",
      "('informative', 'tesla begins making good elon musks promise help puerto rico rebuild')\n",
      "('informative', 'hurricane irma turn trees brown brevard gt')\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "from torch.utils.data.dataset import random_split\n",
    "from random import sample\n",
    "\n",
    "# Proporciones\n",
    "train_ratio = 0.75\n",
    "dev_ratio = 0.15\n",
    "test_ratio = 0.15\n",
    "\n",
    "# Longitudes\n",
    "train_len = int(len(dataset) * train_ratio)\n",
    "dev_len = int(len(dataset) * dev_ratio)\n",
    "test_len = len(dataset) - train_len - dev_len  # para evitar problemas de redondeo\n",
    "\n",
    "# Dividimos el dataset\n",
    "train_split, dev_split, test_split = random_split(dataset, [train_len, dev_len, test_len])\n",
    "\n",
    "print(f\"Tamaño del set de entrenamiento: {len(train_split)}\")\n",
    "print(f\"Tamaño del set de validación: {len(dev_split)}\")\n",
    "print(f\"Tamaño del set de prueba: {len(test_split)}\")\n",
    "\n",
    "# Mostramos algunos ejemplos\n",
    "print(\"\\nEjemplos del set de entrenamiento:\")\n",
    "for example in sample(list(train_split), 3):\n",
    "    print(example)\n",
    "print(\"\\nEjemplos del set de validación:\")\n",
    "for example in sample(list(dev_split), 3):\n",
    "    print(example)\n",
    "print(\"\\nEjemplos del set de prueba:\")\n",
    "for example in sample(list(test_split), 3):\n",
    "    print(example)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "c1073207",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt_tab to\n",
      "[nltk_data]     C:\\Users\\isaja\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package punkt_tab is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "# Tokenización\n",
    "nltk.download('punkt_tab')\n",
    "from nltk.tokenize import word_tokenize\n",
    "from collections import namedtuple"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "bc457351",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>text</th>\n",
       "      <th>label</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>[social, media, becomes, savior, hurricane, ha...</td>\n",
       "      <td>informative</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>[lets, forget, mexico, incredible, graphic, de...</td>\n",
       "      <td>informative</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>[frankthorp, cruz, yes, harvey, debt, limit, c...</td>\n",
       "      <td>informative</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>[buy, solar, light, amp, spread, word, let, li...</td>\n",
       "      <td>not_informative</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>[thick, smoke, across, northern, california, s...</td>\n",
       "      <td>not_informative</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                text            label\n",
       "0  [social, media, becomes, savior, hurricane, ha...      informative\n",
       "1  [lets, forget, mexico, incredible, graphic, de...      informative\n",
       "2  [frankthorp, cruz, yes, harvey, debt, limit, c...      informative\n",
       "3  [buy, solar, light, amp, spread, word, let, li...  not_informative\n",
       "4  [thick, smoke, across, northern, california, s...  not_informative"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "document = namedtuple('tweet', ['text', 'label'])\n",
    "\n",
    "tokenized_train_set = [document(text=word_tokenize(d[1]), label=d[0]) for d in train_split]\n",
    "train_set = pd.DataFrame(tokenized_train_set)\n",
    "\n",
    "tokenized_dev_set = [document(text=word_tokenize(d[1]), label=d[0]) for d in dev_split]\n",
    "dev_set = pd.DataFrame(tokenized_dev_set)\n",
    "\n",
    "tokenized_test_set = [document(text=word_tokenize(d[1]), label=d[0]) for d in test_split]\n",
    "test_set = pd.DataFrame(tokenized_test_set)\n",
    "\n",
    "test_set.head()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4dd719d1",
   "metadata": {},
   "source": [
    "### **Arquitectura del modelo**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "12b635d1",
   "metadata": {},
   "outputs": [],
   "source": [
    "import gensim\n",
    "from gensim.models import Word2Vec"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "5cb3d57d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "El vocabulario tiene 3066 palabras\n",
      "Algunas palabras aleatorias del vocabulario: ['infrared', 'adorable', 'rocks', 'unit', 'lds', 'sink', 'tracking', 'hour', 'instead', 'proud']\n",
      "Palabras más similares a 'mexico': [('iran', 0.9982720017433167), ('city', 0.9968012571334839), ('iraq', 0.9967494606971741), ('border', 0.9958007335662842), ('earthquake', 0.9954339265823364), ('magnitude', 0.9947237372398376), ('shakes', 0.9923250675201416), ('kermanshah', 0.9917623400688171), ('powerful', 0.991410493850708), ('hits', 0.9908280968666077)]\n",
      "La similitud entre 'help' y 'support' es: 0.9995922446250916\n"
     ]
    }
   ],
   "source": [
    "# Jugando con Word2Vec\n",
    "sentences = train_set['text'].tolist()  # Se pasa a lista de listas de tokens\n",
    "w2v = Word2Vec(sentences, vector_size=300, window=5, min_count=5, workers=4)\n",
    "\n",
    "vocab = list(w2v.wv.key_to_index.keys())\n",
    "\n",
    "palabra_a_buscar = 'mexico'\n",
    "print(f\"El vocabulario tiene {len(vocab)} palabras\")\n",
    "print(f\"Algunas palabras aleatorias del vocabulario: {sample(vocab, 10)}\")\n",
    "\n",
    "# Buscar palabras similares\n",
    "w2v.wv.most_similar(palabra_a_buscar)\n",
    "print(f\"Palabras más similares a '{palabra_a_buscar}': {w2v.wv.most_similar(palabra_a_buscar)}\")\n",
    "\n",
    "# Similitud entre dos palabras\n",
    "palabra1 = 'help'\n",
    "palabra2 = 'support'\n",
    "similarity = w2v.wv.similarity(palabra1, palabra2)\n",
    "print(f\"La similitud entre '{palabra1}' y '{palabra2}' es: {similarity}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "7418d577",
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.keras.callbacks import EarlyStopping\n",
    "from tensorflow.keras import layers, models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "75d36f56",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Construcción del modelo \n",
    "\n",
    "class CNN:\n",
    "    def __init__(self,\n",
    "                 vocab_size,\n",
    "                 embedding_matrix,\n",
    "                 embedding_dim=300,\n",
    "                 max_seq_len=100,\n",
    "                 num_classes=2,\n",
    "                 ):\n",
    "        self.vocab_size = vocab_size\n",
    "        self.embedding_dim = embedding_dim\n",
    "        self.max_seq_len = max_seq_len\n",
    "        self.num_classes = num_classes\n",
    "        self.embedding_matrix = embedding_matrix\n",
    "        self.model = self.build_model()\n",
    "\n",
    "    def build_model(self):\n",
    "        input_layer = layers.Input(shape=(self.max_seq_len,))\n",
    "        x = layers.Embedding(input_dim=self.vocab_size,\n",
    "                                           output_dim=self.embedding_dim,\n",
    "                                           weights=[self.embedding_matrix],\n",
    "                                           input_length=self.max_seq_len,\n",
    "                                           trainable=False)(input_layer)\n",
    "        # convoluciones\n",
    "        conv_blocks = []\n",
    "        conv1 = layers.Conv1D(filters=100,\n",
    "                              kernel_size=2,\n",
    "                              activation='relu',\n",
    "                              padding='same')(x)\n",
    "        # Se toma el valor máximo de cada filtro\n",
    "        pool1 = layers.GlobalMaxPooling1D()(conv1)\n",
    "        conv_blocks.append(pool1)\n",
    "        conv2 = layers.Conv1D(filters=150,\n",
    "                              kernel_size=3,\n",
    "                              activation='relu',\n",
    "                              padding='same')(x)\n",
    "        pool2 = layers.GlobalMaxPooling1D()(conv2)\n",
    "        conv_blocks.append(pool2)\n",
    "        conv3 = layers.Conv1D(filters=200,\n",
    "                              kernel_size=4,\n",
    "                              activation='relu',\n",
    "                              padding='same')(x)\n",
    "        pool3 = layers.GlobalMaxPooling1D()(conv3)\n",
    "        conv_blocks.append(pool3)\n",
    "\n",
    "        # concatenar las salidas de las convoluciones\n",
    "        concat = layers.Concatenate()(conv_blocks)\n",
    "\n",
    "        # dropout para evitar overfitting apagando neuronas aleatoriamente\n",
    "        x = layers.Dropout(0.02)(concat)\n",
    "        \n",
    "        x = layers.Dense(512, activation='relu')(x)\n",
    "        x = layers.Dropout(0.02)(x)\n",
    "        x = layers.Dense(256, activation='relu')(x)\n",
    "        x = layers.Dropout(0.02)(x)\n",
    "        x = layers.Dense(128, activation='relu')(x)\n",
    "        x = layers.Dropout(0.02)(x)\n",
    "        x = layers.Dense(64, activation='relu')(x)\n",
    "        x = layers.Dropout(0.02)(x)\n",
    "        x = layers.Dense(32, activation='relu')(x)\n",
    "        x = layers.Dropout(0.02)(x)\n",
    "\n",
    "        # capa de salida\n",
    "        output_layer = layers.Dense(self.num_classes, activation='softmax')(x)\n",
    "        model = models.Model(inputs=input_layer, outputs=output_layer)\n",
    "        return model\n",
    "    \n",
    "    def compile_model(self):\n",
    "        self.model.compile(optimizer='adam',\n",
    "                           loss='sparse_categorical_crossentropy',\n",
    "                           metrics=['accuracy'])\n",
    "        return self.model\n",
    "    \n",
    "    def train(self, X_train, y_train, X_val, y_val, epochs=10, batch_size=32):\n",
    "        callbacks = [EarlyStopping(\n",
    "            monitor='val_accuracy',\n",
    "            patience=10,\n",
    "            restore_best_weights=True,\n",
    "            verbose=1\n",
    "        )]\n",
    "        history = self.model.fit(X_train, y_train,\n",
    "                                 validation_data=(X_val, y_val),\n",
    "                                 epochs=epochs,\n",
    "                                 batch_size=batch_size)\n",
    "        return history\n",
    "    \n",
    "    def evaluate(self, X_test, y_test):\n",
    "        results = self.model.evaluate(X_test, y_test)\n",
    "        return results\n",
    "\n",
    "    def predict(self, X):\n",
    "        predictions = self.model.predict(X)\n",
    "        return predictions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "2bc3be9d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Corriendo el modelo\n",
    "\n",
    "def embedding_matriz(word_index, w2v_model, embedding_dim=300):\n",
    "    vocab_size = len(word_index) + 1\n",
    "    # matriz de embedding inicializada en ceros\n",
    "    embedding_matrix = np.zeros((vocab_size, embedding_dim))\n",
    "    for word, i in word_index.items():\n",
    "        if word in w2v_model.wv:\n",
    "            # si la palabra está en el vocabulario de word2vec\n",
    "            embedding_matrix[i] = w2v_model.wv[word]\n",
    "        else:\n",
    "            # si la palabra no está, vector aleatorio\n",
    "            embedding_matrix[i] = np.random.normal(0, 0.1, embedding_dim)\n",
    "    return embedding_matrix\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "801c6b4f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>text</th>\n",
       "      <th>label</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>[help, harvey, disaster, relief]</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>[aircanada, staff, helped, us, pack, water, pu...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>[colorado, based, supertanker, takes, californ...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>[great, info, graphic, capturing, harvey, impa...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>[west, bank, flea, market, harvey]</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                text  label\n",
       "0                   [help, harvey, disaster, relief]      1\n",
       "1  [aircanada, staff, helped, us, pack, water, pu...      1\n",
       "2  [colorado, based, supertanker, takes, californ...      1\n",
       "3  [great, info, graphic, capturing, harvey, impa...      1\n",
       "4                 [west, bank, flea, market, harvey]      0"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# pasando a etiquetas numéricas\n",
    "label_mapping = {'informative': 1, 'not_informative': 0}\n",
    "test_set['label'] = test_set['label'].map(label_mapping)\n",
    "dev_set['label'] = dev_set['label'].map(label_mapping)\n",
    "train_set['label'] = train_set['label'].map(label_mapping)\n",
    "train_set.head()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
